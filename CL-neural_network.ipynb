{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37564bit61209d9bcfea452998e2771b9ee1a7fb",
   "display_name": "Python 3.7.5 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Neural Network\n",
    "A partire dal dataset processato applichiamo una neural network per predirre la classe di appartenenza\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "df = pd.read_csv(\"dataset/CL-dataset.csv\",sep='\\t',decimal=\",\",index_col=0)\n",
    "df = df.drop(columns=[\"CustomerID\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   Imax         E    PIL          Mb      MeanD     EDate  class\n",
       "0    12  4.335643  2.567  156.860294   0.000000  0.000000      2\n",
       "1    32  6.503112  2.567  356.232222  35.000000  2.226424      2\n",
       "2   360  6.504979  2.752  442.969333  24.266667  2.235084      2\n",
       "3    80  4.494680  2.567  189.650000  55.400000  1.177965      1\n",
       "4    32  0.000000  2.567  292.000000  13.000000  0.918296      1"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Imax</th>\n      <th>E</th>\n      <th>PIL</th>\n      <th>Mb</th>\n      <th>MeanD</th>\n      <th>EDate</th>\n      <th>class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>12</td>\n      <td>4.335643</td>\n      <td>2.567</td>\n      <td>156.860294</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>32</td>\n      <td>6.503112</td>\n      <td>2.567</td>\n      <td>356.232222</td>\n      <td>35.000000</td>\n      <td>2.226424</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>360</td>\n      <td>6.504979</td>\n      <td>2.752</td>\n      <td>442.969333</td>\n      <td>24.266667</td>\n      <td>2.235084</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>80</td>\n      <td>4.494680</td>\n      <td>2.567</td>\n      <td>189.650000</td>\n      <td>55.400000</td>\n      <td>1.177965</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>32</td>\n      <td>0.000000</td>\n      <td>2.567</td>\n      <td>292.000000</td>\n      <td>13.000000</td>\n      <td>0.918296</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 90
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "label = df.pop('class')\n",
    "train_set, test_set, train_label, test_label = train_test_split(df, label, stratify =label, test_size=0.30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam, SGD\n",
    "from tensorflow.keras.layers import Dense, Dropout, InputLayer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras import utils\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(train_label)\n",
    "encoded_Y = encoder.transform(train_label)\n",
    "train_label = utils.to_categorical(encoded_Y)\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(test_label)\n",
    "encoded_Y = encoder.transform(test_label)\n",
    "test_label = utils.to_categorical(encoded_Y)"
   ]
  },
  {
   "source": [
    "## Grid Search"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "grid_params = {\n",
    "                'hidden_layers': [1, 2,3],\n",
    "                'hidden_units': [64,128,256],\n",
    "                'act_funct': ['relu', 'tanh'],\n",
    "                'learning_rate': [1e-6, 1e-5, 1e-7],\n",
    "                'optimizer': [Adam, RMSprop]\n",
    "            }\n",
    "keys, values = zip(*grid_params.items())\n",
    "params_list = [dict(zip(keys, v)) for v in product(*values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " it was called on an input with incompatible shape (None, 6).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 1, 6) for input Tensor(\"flatten_43_input:0\", shape=(None, 1, 6), dtype=float32), but it was called on an input with incompatible shape (None, 6).\n",
      "68/98 [===================>..........] - ETA: 0s - loss: 47.9188 - accuracy: 0.3028WARNING:tensorflow:Model was constructed with shape (None, 1, 6) for input Tensor(\"flatten_43_input:0\", shape=(None, 1, 6), dtype=float32), but it was called on an input with incompatible shape (None, 6).\n",
      "98/98 [==============================] - 0s 4ms/step - loss: 47.9067 - accuracy: 0.2957 - val_loss: 39.2371 - val_accuracy: 0.2394\n",
      "Epoch 2/2\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 48.7246 - accuracy: 0.2884 - val_loss: 39.0191 - val_accuracy: 0.2394\n",
      "Epoch 1/2\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 1, 6) for input Tensor(\"flatten_44_input:0\", shape=(None, 1, 6), dtype=float32), but it was called on an input with incompatible shape (None, 6).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 1, 6) for input Tensor(\"flatten_44_input:0\", shape=(None, 1, 6), dtype=float32), but it was called on an input with incompatible shape (None, 6).\n",
      "87/98 [=========================>....] - ETA: 0s - loss: 92.2452 - accuracy: 0.4648WARNING:tensorflow:Model was constructed with shape (None, 1, 6) for input Tensor(\"flatten_44_input:0\", shape=(None, 1, 6), dtype=float32), but it was called on an input with incompatible shape (None, 6).\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 95.3725 - accuracy: 0.4678 - val_loss: 80.9836 - val_accuracy: 0.4891\n",
      "Epoch 2/2\n",
      "98/98 [==============================] - 0s 1ms/step - loss: 90.4377 - accuracy: 0.4662 - val_loss: 77.2487 - val_accuracy: 0.4853\n",
      "Epoch 1/2\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 1, 6) for input Tensor(\"flatten_45_input:0\", shape=(None, 1, 6), dtype=float32), but it was called on an input with incompatible shape (None, 6).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 1, 6) for input Tensor(\"flatten_45_input:0\", shape=(None, 1, 6), dtype=float32), but it was called on an input with incompatible shape (None, 6).\n",
      "50/98 [==============>...............] - ETA: 0s - loss: 116.7798 - accuracy: 0.1650WARNING:tensorflow:Model was constructed with shape (None, 1, 6) for input Tensor(\"flatten_45_input:0\", shape=(None, 1, 6), dtype=float32), but it was called on an input with incompatible shape (None, 6).\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 112.5337 - accuracy: 0.1685 - val_loss: 109.2744 - val_accuracy: 0.1805\n",
      "Epoch 2/2\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 109.1075 - accuracy: 0.1689 - val_loss: 105.2613 - val_accuracy: 0.1805\n",
      "Epoch 1/2\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 1, 6) for input Tensor(\"flatten_46_input:0\", shape=(None, 1, 6), dtype=float32), but it was called on an input with incompatible shape (None, 6).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 1, 6) for input Tensor(\"flatten_46_input:0\", shape=(None, 1, 6), dtype=float32), but it was called on an input with incompatible shape (None, 6).\n",
      "95/98 [============================>.] - ETA: 0s - loss: 56.7765 - accuracy: 0.3967WARNING:tensorflow:Model was constructed with shape (None, 1, 6) for input Tensor(\"flatten_46_input:0\", shape=(None, 1, 6), dtype=float32), but it was called on an input with incompatible shape (None, 6).\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 56.8525 - accuracy: 0.3963 - val_loss: 39.3433 - val_accuracy: 0.5109\n",
      "Epoch 2/2\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 55.9722 - accuracy: 0.3938 - val_loss: 39.3114 - val_accuracy: 0.5109\n",
      "Epoch 1/2\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 1, 6) for input Tensor(\"flatten_47_input:0\", shape=(None, 1, 6), dtype=float32), but it was called on an input with incompatible shape (None, 6).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 1, 6) for input Tensor(\"flatten_47_input:0\", shape=(None, 1, 6), dtype=float32), but it was called on an input with incompatible shape (None, 6).\n",
      "77/98 [======================>.......] - ETA: 0s - loss: 72.4992 - accuracy: 0.2829WARNING:tensorflow:Model was constructed with shape (None, 1, 6) for input Tensor(\"flatten_47_input:0\", shape=(None, 1, 6), dtype=float32), but it was called on an input with incompatible shape (None, 6).\n",
      "98/98 [==============================] - 1s 5ms/step - loss: 72.7880 - accuracy: 0.2839 - val_loss: 62.2379 - val_accuracy: 0.2241\n",
      "Epoch 2/2\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 72.4465 - accuracy: 0.2730 - val_loss: 62.2116 - val_accuracy: 0.2241\n",
      "Epoch 1/2\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 1, 6) for input Tensor(\"flatten_48_input:0\", shape=(None, 1, 6), dtype=float32), but it was called on an input with incompatible shape (None, 6).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 1, 6) for input Tensor(\"flatten_48_input:0\", shape=(None, 1, 6), dtype=float32), but it was called on an input with incompatible shape (None, 6).\n",
      "80/98 [=======================>......] - ETA: 0s - loss: 1.2860 - accuracy: 0.4203WARNING:tensorflow:Model was constructed with shape (None, 1, 6) for input Tensor(\"flatten_48_input:0\", shape=(None, 1, 6), dtype=float32), but it was called on an input with incompatible shape (None, 6).\n",
      "98/98 [==============================] - 0s 4ms/step - loss: 1.2866 - accuracy: 0.4207 - val_loss: 1.1690 - val_accuracy: 0.4738\n",
      "Epoch 2/2\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 1.2647 - accuracy: 0.4367 - val_loss: 1.1678 - val_accuracy: 0.4738\n",
      "Epoch 1/2\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 1, 6) for input Tensor(\"flatten_49_input:0\", shape=(None, 1, 6), dtype=float32), but it was called on an input with incompatible shape (None, 6).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 1, 6) for input Tensor(\"flatten_49_input:0\", shape=(None, 1, 6), dtype=float32), but it was called on an input with incompatible shape (None, 6).\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 1.4277 - accuracy: 0.4966WARNING:tensorflow:Model was constructed with shape (None, 1, 6) for input Tensor(\"flatten_49_input:0\", shape=(None, 1, 6), dtype=float32), but it was called on an input with incompatible shape (None, 6).\n",
      "98/98 [==============================] - 0s 4ms/step - loss: 1.4310 - accuracy: 0.4960 - val_loss: 1.4004 - val_accuracy: 0.5032\n",
      "Epoch 2/2\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 1.4303 - accuracy: 0.4931 - val_loss: 1.3966 - val_accuracy: 0.5019\n",
      "Epoch 1/2\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 1, 6) for input Tensor(\"flatten_50_input:0\", shape=(None, 1, 6), dtype=float32), but it was called on an input with incompatible shape (None, 6).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 1, 6) for input Tensor(\"flatten_50_input:0\", shape=(None, 1, 6), dtype=float32), but it was called on an input with incompatible shape (None, 6).\n",
      "71/98 [====================>.........] - ETA: 0s - loss: 1.2915 - accuracy: 0.3649WARNING:tensorflow:Model was constructed with shape (None, 1, 6) for input Tensor(\"flatten_50_input:0\", shape=(None, 1, 6), dtype=float32), but it was called on an input with incompatible shape (None, 6).\n",
      "WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0010s vs `on_test_batch_end` time: 0.0020s). Check your callbacks.\n",
      "98/98 [==============================] - 0s 5ms/step - loss: 1.2916 - accuracy: 0.3585 - val_loss: 1.1536 - val_accuracy: 0.3086\n",
      "Epoch 2/2\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 1.2770 - accuracy: 0.3701 - val_loss: 1.1389 - val_accuracy: 0.3291\n",
      "Epoch 1/2\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 1, 6) for input Tensor(\"flatten_51_input:0\", shape=(None, 1, 6), dtype=float32), but it was called on an input with incompatible shape (None, 6).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 1, 6) for input Tensor(\"flatten_51_input:0\", shape=(None, 1, 6), dtype=float32), but it was called on an input with incompatible shape (None, 6).\n",
      "88/98 [=========================>....] - ETA: 0s - loss: 1.3524 - accuracy: 0.3438WARNING:tensorflow:Model was constructed with shape (None, 1, 6) for input Tensor(\"flatten_51_input:0\", shape=(None, 1, 6), dtype=float32), but it was called on an input with incompatible shape (None, 6).\n",
      "98/98 [==============================] - 1s 5ms/step - loss: 1.3504 - accuracy: 0.3448 - val_loss: 1.2627 - val_accuracy: 0.3905\n",
      "Epoch 2/2\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 1.3572 - accuracy: 0.3345 - val_loss: 1.2534 - val_accuracy: 0.3931\n",
      "Epoch 1/2\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 1, 6) for input Tensor(\"flatten_52_input:0\", shape=(None, 1, 6), dtype=float32), but it was called on an input with incompatible shape (None, 6).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 1, 6) for input Tensor(\"flatten_52_input:0\", shape=(None, 1, 6), dtype=float32), but it was called on an input with incompatible shape (None, 6).\n",
      "88/98 [=========================>....] - ETA: 0s - loss: 1.4942 - accuracy: 0.3697WARNING:tensorflow:Model was constructed with shape (None, 1, 6) for input Tensor(\"flatten_52_input:0\", shape=(None, 1, 6), dtype=float32), but it was called on an input with incompatible shape (None, 6).\n",
      "98/98 [==============================] - 0s 4ms/step - loss: 1.5032 - accuracy: 0.3714 - val_loss: 1.4455 - val_accuracy: 0.3380\n",
      "Epoch 2/2\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 1.4729 - accuracy: 0.3957 - val_loss: 1.4451 - val_accuracy: 0.3380\n",
      "Epoch 1/2\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 1, 6) for input Tensor(\"flatten_53_input:0\", shape=(None, 1, 6), dtype=float32), but it was called on an input with incompatible shape (None, 6).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 1, 6) for input Tensor(\"flatten_53_input:0\", shape=(None, 1, 6), dtype=float32), but it was called on an input with incompatible shape (None, 6).\n",
      "70/98 [====================>.........] - ETA: 0s - loss: 1.2859 - accuracy: 0.4103WARNING:tensorflow:Model was constructed with shape (None, 1, 6) for input Tensor(\"flatten_53_input:0\", shape=(None, 1, 6), dtype=float32), but it was called on an input with incompatible shape (None, 6).\n",
      "98/98 [==============================] - 0s 5ms/step - loss: 1.2936 - accuracy: 0.4133 - val_loss: 1.2256 - val_accuracy: 0.3585\n",
      "Epoch 2/2\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 1.3196 - accuracy: 0.4005 - val_loss: 1.2252 - val_accuracy: 0.3585\n",
      "Epoch 1/2\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 1, 6) for input Tensor(\"flatten_54_input:0\", shape=(None, 1, 6), dtype=float32), but it was called on an input with incompatible shape (None, 6).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 1, 6) for input Tensor(\"flatten_54_input:0\", shape=(None, 1, 6), dtype=float32), but it was called on an input with incompatible shape (None, 6).\n",
      "83/98 [========================>.....] - ETA: 0s - loss: 55.2556 - accuracy: 0.4880WARNING:tensorflow:Model was constructed with shape (None, 1, 6) for input Tensor(\"flatten_54_input:0\", shape=(None, 1, 6), dtype=float32), but it was called on an input with incompatible shape (None, 6).\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 54.5048 - accuracy: 0.4848 - val_loss: 49.1373 - val_accuracy: 0.4994\n",
      "Epoch 2/2\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 55.7895 - accuracy: 0.4675 - val_loss: 48.5472 - val_accuracy: 0.4994\n",
      "Epoch 1/2\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 1, 6) for input Tensor(\"flatten_55_input:0\", shape=(None, 1, 6), dtype=float32), but it was called on an input with incompatible shape (None, 6).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 1, 6) for input Tensor(\"flatten_55_input:0\", shape=(None, 1, 6), dtype=float32), but it was called on an input with incompatible shape (None, 6).\n",
      "92/98 [===========================>..] - ETA: 0s - loss: 94.6142 - accuracy: 0.5095WARNING:tensorflow:Model was constructed with shape (None, 1, 6) for input Tensor(\"flatten_55_input:0\", shape=(None, 1, 6), dtype=float32), but it was called on an input with incompatible shape (None, 6).\n",
      "98/98 [==============================] - 0s 4ms/step - loss: 95.0967 - accuracy: 0.5123 - val_loss: 88.1361 - val_accuracy: 0.5134\n",
      "Epoch 2/2\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 93.1967 - accuracy: 0.5123 - val_loss: 87.5106 - val_accuracy: 0.5134\n",
      "Epoch 1/2\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 1, 6) for input Tensor(\"flatten_56_input:0\", shape=(None, 1, 6), dtype=float32), but it was called on an input with incompatible shape (None, 6).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 1, 6) for input Tensor(\"flatten_56_input:0\", shape=(None, 1, 6), dtype=float32), but it was called on an input with incompatible shape (None, 6).\n",
      "94/98 [===========================>..] - ETA: 0s - loss: 41.6973 - accuracy: 0.2201WARNING:tensorflow:Model was constructed with shape (None, 1, 6) for input Tensor(\"flatten_56_input:0\", shape=(None, 1, 6), dtype=float32), but it was called on an input with incompatible shape (None, 6).\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 41.5748 - accuracy: 0.2195 - val_loss: 27.4845 - val_accuracy: 0.1805\n",
      "Epoch 2/2\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 38.9675 - accuracy: 0.2140 - val_loss: 24.3120 - val_accuracy: 0.1805\n",
      "Epoch 1/2\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 1, 6) for input Tensor(\"flatten_57_input:0\", shape=(None, 1, 6), dtype=float32), but it was called on an input with incompatible shape (None, 6).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 1, 6) for input Tensor(\"flatten_57_input:0\", shape=(None, 1, 6), dtype=float32), but it was called on an input with incompatible shape (None, 6).\n",
      "86/98 [=========================>....] - ETA: 0s - loss: 54.8046 - accuracy: 0.3387WARNING:tensorflow:Model was constructed with shape (None, 1, 6) for input Tensor(\"flatten_57_input:0\", shape=(None, 1, 6), dtype=float32), but it was called on an input with incompatible shape (None, 6).\n",
      "98/98 [==============================] - 0s 3ms/step - loss: 54.7776 - accuracy: 0.3358 - val_loss: 48.4752 - val_accuracy: 0.3060\n",
      "Epoch 2/2\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 51.5896 - accuracy: 0.3403 - val_loss: 44.2751 - val_accuracy: 0.3060\n",
      "Epoch 1/2\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 1, 6) for input Tensor(\"flatten_58_input:0\", shape=(None, 1, 6), dtype=float32), but it was called on an input with incompatible shape (None, 6).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 1, 6) for input Tensor(\"flatten_58_input:0\", shape=(None, 1, 6), dtype=float32), but it was called on an input with incompatible shape (None, 6).\n",
      "76/98 [======================>.......] - ETA: 0s - loss: 58.8840 - accuracy: 0.3392WARNING:tensorflow:Model was constructed with shape (None, 1, 6) for input Tensor(\"flatten_58_input:0\", shape=(None, 1, 6), dtype=float32), but it was called on an input with incompatible shape (None, 6).\n",
      "98/98 [==============================] - 0s 4ms/step - loss: 59.6733 - accuracy: 0.3441 - val_loss: 43.9797 - val_accuracy: 0.4008\n",
      "Epoch 2/2\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 60.4457 - accuracy: 0.3438 - val_loss: 43.9434 - val_accuracy: 0.4008\n",
      "Epoch 1/2\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 1, 6) for input Tensor(\"flatten_59_input:0\", shape=(None, 1, 6), dtype=float32), but it was called on an input with incompatible shape (None, 6).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 1, 6) for input Tensor(\"flatten_59_input:0\", shape=(None, 1, 6), dtype=float32), but it was called on an input with incompatible shape (None, 6).\n",
      "94/98 [===========================>..] - ETA: 0s - loss: 60.5307 - accuracy: 0.3195WARNING:tensorflow:Model was constructed with shape (None, 1, 6) for input Tensor(\"flatten_59_input:0\", shape=(None, 1, 6), dtype=float32), but it was called on an input with incompatible shape (None, 6).\n",
      "98/98 [==============================] - 0s 4ms/step - loss: 60.6186 - accuracy: 0.3207 - val_loss: 50.2709 - val_accuracy: 0.2650\n",
      "Epoch 2/2\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 58.8740 - accuracy: 0.3236 - val_loss: 50.2388 - val_accuracy: 0.2650\n",
      "Epoch 1/2\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 1, 6) for input Tensor(\"flatten_60_input:0\", shape=(None, 1, 6), dtype=float32), but it was called on an input with incompatible shape (None, 6).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 1, 6) for input Tensor(\"flatten_60_input:0\", shape=(None, 1, 6), dtype=float32), but it was called on an input with incompatible shape (None, 6).\n",
      "72/98 [=====================>........] - ETA: 0s - loss: 1.1985 - accuracy: 0.4414WARNING:tensorflow:Model was constructed with shape (None, 1, 6) for input Tensor(\"flatten_60_input:0\", shape=(None, 1, 6), dtype=float32), but it was called on an input with incompatible shape (None, 6).\n",
      "98/98 [==============================] - 0s 4ms/step - loss: 1.2153 - accuracy: 0.4460 - val_loss: 1.1000 - val_accuracy: 0.4635\n",
      "Epoch 2/2\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 1.2042 - accuracy: 0.4460 - val_loss: 1.0973 - val_accuracy: 0.4648\n",
      "Epoch 1/2\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 1, 6) for input Tensor(\"flatten_61_input:0\", shape=(None, 1, 6), dtype=float32), but it was called on an input with incompatible shape (None, 6).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 1, 6) for input Tensor(\"flatten_61_input:0\", shape=(None, 1, 6), dtype=float32), but it was called on an input with incompatible shape (None, 6).\n",
      "83/98 [========================>.....] - ETA: 0s - loss: 2.1424 - accuracy: 0.2850WARNING:tensorflow:Model was constructed with shape (None, 1, 6) for input Tensor(\"flatten_61_input:0\", shape=(None, 1, 6), dtype=float32), but it was called on an input with incompatible shape (None, 6).\n",
      "98/98 [==============================] - 0s 4ms/step - loss: 2.1337 - accuracy: 0.2871 - val_loss: 2.0618 - val_accuracy: 0.3355\n",
      "Epoch 2/2\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 2.1376 - accuracy: 0.2768 - val_loss: 2.0497 - val_accuracy: 0.3367\n",
      "Epoch 1/2\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 1, 6) for input Tensor(\"flatten_62_input:0\", shape=(None, 1, 6), dtype=float32), but it was called on an input with incompatible shape (None, 6).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 1, 6) for input Tensor(\"flatten_62_input:0\", shape=(None, 1, 6), dtype=float32), but it was called on an input with incompatible shape (None, 6).\n",
      "83/98 [========================>.....] - ETA: 0s - loss: 1.2915 - accuracy: 0.4631WARNING:tensorflow:Model was constructed with shape (None, 1, 6) for input Tensor(\"flatten_62_input:0\", shape=(None, 1, 6), dtype=float32), but it was called on an input with incompatible shape (None, 6).\n",
      "98/98 [==============================] - 0s 5ms/step - loss: 1.2978 - accuracy: 0.4636 - val_loss: 1.1839 - val_accuracy: 0.4917\n",
      "Epoch 2/2\n",
      "98/98 [==============================] - 0s 2ms/step - loss: 1.2360 - accuracy: 0.4748 - val_loss: 1.1517 - val_accuracy: 0.5160\n",
      "Epoch 1/2\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 1, 6) for input Tensor(\"flatten_63_input:0\", shape=(None, 1, 6), dtype=float32), but it was called on an input with incompatible shape (None, 6).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 1, 6) for input Tensor(\"flatten_63_input:0\", shape=(None, 1, 6), dtype=float32), but it was called on an input with incompatible shape (None, 6).\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-95-22ecb8adbbe9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;31m# Allenamento\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[1;31m#y_train = np.asarray(train_label).astype('float32').reshape((-1,1))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m     \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_label\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;31m#Plot\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    838\u001b[0m         \u001b[1;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    839\u001b[0m         \u001b[1;31m# stateless function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 840\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    841\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    842\u001b[0m       \u001b[0mcanon_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2826\u001b[0m     \u001b[1;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2828\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2829\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3212\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3213\u001b[1;33m       \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3214\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3215\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3073\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3074\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3075\u001b[1;33m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[0;32m   3076\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3077\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    984\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    985\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 986\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    987\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    988\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    598\u001b[0m         \u001b[1;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    599\u001b[0m         \u001b[1;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 600\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    601\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mref\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    602\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    967\u001b[0m                     \u001b[0mrecursive\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    968\u001b[0m                     \u001b[0moptional_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mautograph_options\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 969\u001b[1;33m                     \u001b[0muser_requested\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    970\u001b[0m                 ))\n\u001b[0;32m    971\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[1;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[0;32m    594\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    595\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 596\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconverted_f\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0meffective_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    597\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    598\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconverted_f\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0meffective_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     14\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m                     \u001b[0mretval_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m                 \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[1;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[0;32m    455\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mconversion\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_in_whitelist_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Whitelisted %s: from cache'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_call_unconverted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol_status_ctx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDISABLED\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36m_call_unconverted\u001b[1;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[0;32m    338\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    339\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 340\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    342\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mstep_function\u001b[1;34m(model, iterator)\u001b[0m\n\u001b[0;32m    794\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    795\u001b[0m       \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 796\u001b[1;33m       \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute_strategy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    797\u001b[0m       outputs = reduce_per_replica(\n\u001b[0;32m    798\u001b[0m           outputs, self.distribute_strategy, reduction='first')\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   1209\u001b[0m       fn = autograph.tf_convert(\n\u001b[0;32m   1210\u001b[0m           fn, autograph_ctx.control_status_ctx(), convert_by_default=False)\n\u001b[1;32m-> 1211\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extended\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall_for_each_replica\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1213\u001b[0m   \u001b[1;31m# TODO(b/151224785): Remove deprecated alias.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py\u001b[0m in \u001b[0;36mcall_for_each_replica\u001b[1;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[0;32m   2583\u001b[0m       \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2584\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_container_strategy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2585\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_for_each_replica\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2586\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2587\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_for_each_replica\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py\u001b[0m in \u001b[0;36m_call_for_each_replica\u001b[1;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[0;32m   2943\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_container_strategy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2944\u001b[0m         replica_id_in_sync_group=constant_op.constant(0, dtypes.int32)):\n\u001b[1;32m-> 2945\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2946\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2947\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_reduce_to\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdestinations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexperimental_hints\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    253\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mconversion_ctx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 255\u001b[1;33m           \u001b[1;32mreturn\u001b[0m \u001b[0mconverted_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    256\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ag_error_metadata'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[1;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[0;32m    530\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muser_requested\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mconversion\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_whitelisted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_call_unconverted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m   \u001b[1;31m# internal_convert_user_code is for example turned off when issuing a dynamic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36m_call_unconverted\u001b[1;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    338\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 339\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    340\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mrun_step\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    787\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m       \u001b[1;32mdef\u001b[0m \u001b[0mrun_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 789\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    790\u001b[0m         \u001b[1;31m# Ensure counter is updated only if `train_step` succeeds.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_minimum_control_deps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_step\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    755\u001b[0m     \u001b[1;31m# such as loss scaling and gradient clipping.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    756\u001b[0m     _minimize(self.distribute_strategy, tape, self.optimizer, loss,\n\u001b[1;32m--> 757\u001b[1;33m               self.trainable_variables)\n\u001b[0m\u001b[0;32m    758\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    759\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompiled_metrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_minimize\u001b[1;34m(strategy, tape, optimizer, loss, trainable_variables)\u001b[0m\n\u001b[0;32m   2743\u001b[0m       optimizer.apply_gradients(\n\u001b[0;32m   2744\u001b[0m           \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2745\u001b[1;33m           experimental_aggregate_gradients=False)\n\u001b[0m\u001b[0;32m   2746\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2747\u001b[0m       \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py\u001b[0m in \u001b[0;36mapply_gradients\u001b[1;34m(self, grads_and_vars, name, experimental_aggregate_gradients)\u001b[0m\n\u001b[0;32m    547\u001b[0m           \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    548\u001b[0m           kwargs={\n\u001b[1;32m--> 549\u001b[1;33m               \u001b[1;34m\"name\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    550\u001b[0m           })\n\u001b[0;32m    551\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py\u001b[0m in \u001b[0;36mmerge_call\u001b[1;34m(self, merge_fn, args, kwargs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m     merge_fn = autograph.tf_convert(\n\u001b[0;32m   2714\u001b[0m         merge_fn, autograph_ctx.control_status_ctx(), convert_by_default=False)\n\u001b[1;32m-> 2715\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_merge_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmerge_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_merge_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmerge_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py\u001b[0m in \u001b[0;36m_merge_call\u001b[1;34m(self, merge_fn, args, kwargs)\u001b[0m\n\u001b[0;32m   2720\u001b[0m         distribution_strategy_context._CrossReplicaThreadMode(self._strategy))  # pylint: disable=protected-access\n\u001b[0;32m   2721\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2722\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmerge_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_strategy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2723\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2724\u001b[0m       \u001b[0m_pop_per_thread_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    253\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mconversion_ctx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 255\u001b[1;33m           \u001b[1;32mreturn\u001b[0m \u001b[0mconverted_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    256\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ag_error_metadata'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[1;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[0;32m    481\u001b[0m         \u001b[0mnew_kwargs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    482\u001b[0m         \u001b[0mcaller_fn_scope\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcaller_fn_scope\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 483\u001b[1;33m         options=options)\n\u001b[0m\u001b[0;32m    484\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    485\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0minspect_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misbuiltin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[1;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[0;32m    455\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mconversion\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_in_whitelist_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Whitelisted %s: from cache'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_call_unconverted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol_status_ctx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDISABLED\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36m_call_unconverted\u001b[1;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    338\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 339\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    340\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py\u001b[0m in \u001b[0;36m_distributed_apply\u001b[1;34m(self, distribution, grads_and_vars, name, apply_state)\u001b[0m\n\u001b[0;32m    631\u001b[0m                               \"update_\" + var.op.name, skip_on_eager=True):\n\u001b[0;32m    632\u001b[0m             update_ops.extend(distribution.extended.update(\n\u001b[1;32m--> 633\u001b[1;33m                 var, apply_grad_to_update_var, args=(grad,), group=False))\n\u001b[0m\u001b[0;32m    634\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    635\u001b[0m       any_symbolic = any(isinstance(i, ops.Operation) or\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[0;32m   2298\u001b[0m         fn, autograph_ctx.control_status_ctx(), convert_by_default=False)\n\u001b[0;32m   2299\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_container_strategy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2300\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2301\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2302\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py\u001b[0m in \u001b[0;36m_update\u001b[1;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[0;32m   2953\u001b[0m     \u001b[1;31m# The implementations of _update() and _update_non_slot() are identical\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2954\u001b[0m     \u001b[1;31m# except _update() passes `var` as the first argument to `fn()`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2955\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_non_slot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2956\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2957\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_update_non_slot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolocate_with\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshould_group\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py\u001b[0m in \u001b[0;36m_update_non_slot\u001b[1;34m(self, colocate_with, fn, args, kwargs, should_group)\u001b[0m\n\u001b[0;32m   2959\u001b[0m     \u001b[1;31m# once that value is used for something.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2960\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mUpdateContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2961\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2962\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mshould_group\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2963\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    253\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mconversion_ctx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 255\u001b[1;33m           \u001b[1;32mreturn\u001b[0m \u001b[0mconverted_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    256\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ag_error_metadata'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[1;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[0;32m    455\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mconversion\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_in_whitelist_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Whitelisted %s: from cache'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_call_unconverted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol_status_ctx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDISABLED\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36m_call_unconverted\u001b[1;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    338\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 339\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    340\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py\u001b[0m in \u001b[0;36mapply_grad_to_update_var\u001b[1;34m(var, grad)\u001b[0m\n\u001b[0;32m    606\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;34m\"apply_state\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dense_apply_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    607\u001b[0m         \u001b[0mapply_kwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"apply_state\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mapply_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 608\u001b[1;33m       \u001b[0mupdate_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_resource_apply_dense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mapply_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    609\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mvar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstraint\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    610\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mupdate_op\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\rmsprop.py\u001b[0m in \u001b[0;36m_resource_apply_dense\u001b[1;34m(self, grad, var, apply_state)\u001b[0m\n\u001b[0;32m    207\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m       rms_t = (coefficients[\"rho\"] * rms +\n\u001b[1;32m--> 209\u001b[1;33m                coefficients[\"one_minus_rho\"] * math_ops.square(grad))\n\u001b[0m\u001b[0;32m    210\u001b[0m       \u001b[0mrms_t\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstate_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massign\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrms_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_locking\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_use_locking\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m       \u001b[0mdenom_t\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrms_t\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36mbinary_op_wrapper\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m   1122\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1124\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1125\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1126\u001b[0m         \u001b[1;31m# Even if dispatching the op failed, the RHS may be a tensor aware\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36m_add_dispatch\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m   1444\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1445\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1446\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_v2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1447\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1448\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\u001b[0m in \u001b[0;36madd_v2\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m    494\u001b[0m   \u001b[1;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    495\u001b[0m   _, _, _op, _outputs = _op_def_library._apply_op_helper(\n\u001b[1;32m--> 496\u001b[1;33m         \"AddV2\", x=x, y=y, name=name)\n\u001b[0m\u001b[0;32m    497\u001b[0m   \u001b[0m_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0m_execute\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmust_record_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[1;34m(op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    742\u001b[0m       op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n\u001b[0;32m    743\u001b[0m                                  \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscope\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 744\u001b[1;33m                                  attrs=attr_protos, op_def=op_def)\n\u001b[0m\u001b[0;32m    745\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    746\u001b[0m     \u001b[1;31m# `outputs` is returned as a separate return value so that the output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36m_create_op_internal\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[0;32m    591\u001b[0m     return super(FuncGraph, self)._create_op_internal(  # pylint: disable=protected-access\n\u001b[0;32m    592\u001b[0m         \u001b[0mop_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 593\u001b[1;33m         compute_device)\n\u001b[0m\u001b[0;32m    594\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    595\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mcapture\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_create_op_internal\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[0;32m   3483\u001b[0m           \u001b[0minput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3484\u001b[0m           \u001b[0moriginal_op\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3485\u001b[1;33m           op_def=op_def)\n\u001b[0m\u001b[0;32m   3486\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_op_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompute_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompute_device\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3487\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[0;32m   1973\u001b[0m         \u001b[0mop_def\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_op_def\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1974\u001b[0m       self._c_op = _create_c_op(self._graph, node_def, inputs,\n\u001b[1;32m-> 1975\u001b[1;33m                                 control_input_ops, op_def)\n\u001b[0m\u001b[0;32m   1976\u001b[0m       \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1977\u001b[0m     \u001b[1;31m# pylint: enable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[1;34m(graph, node_def, inputs, control_inputs, op_def)\u001b[0m\n\u001b[0;32m   1785\u001b[0m   op_desc = pywrap_tf_session.TF_NewOperation(graph._c_graph,\n\u001b[0;32m   1786\u001b[0m                                               \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1787\u001b[1;33m                                               compat.as_str(node_def.name))\n\u001b[0m\u001b[0;32m   1788\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mnode_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1789\u001b[0m     \u001b[0mpywrap_tf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_SetDevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop_desc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for idx, params in enumerate(params_list):\n",
    "\n",
    "    #Parametri\n",
    "    hidden_layers = params['hidden_layers']\n",
    "    hidden_units = params['hidden_units']\n",
    "    act_funct = params['act_funct']\n",
    "    learning_rate = params['learning_rate']\n",
    "    optimizer = params['optimizer']\n",
    "\n",
    "    #Creazione del modello\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.Flatten(input_shape=(1, 6)))\n",
    "\n",
    "    for i in range(0, hidden_layers):\n",
    "        model.add(Dense(hidden_units, activation = act_funct))\n",
    "        model.add(tf.keras.layers.Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(3, activation = 'softmax')) #Inserire softmax di 3 neuroni\n",
    "\n",
    "    model.compile(optimizer=optimizer(learning_rate=learning_rate),\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "    # Allenamento\n",
    "    #y_train = np.asarray(train_label).astype('float32').reshape((-1,1))\n",
    "    history = model.fit(train_set, train_label,epochs=2,validation_split=0.2)\n",
    "\n",
    "    #Plot\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    epochs = range(1, len(acc) + 1)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(epochs, acc, 'b', label='Training Acc')\n",
    "    plt.plot(epochs, val_acc, 'r', label='Validation Acc')\n",
    "    plt.title(\"LR: \"+str(learning_rate)+\" ACT: \"+act_funct+\" LAYER: \"+ str(hidden_layers)+ \" UNITS: \"+ str(hidden_units))\n",
    "\n",
    "\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Acc')\n",
    "    plt.legend()\n",
    "    plt.savefig(\"./nn_grid/plt\"+str(idx)+\".png\")"
   ]
  },
  {
   "source": [
    "## Allenamento finale"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(1, 6)),\n",
    "  tf.keras.layers.Dense(128, activation='sigmoid'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(128, activation='sigmoid'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(128, activation='sigmoid'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(3, activation='softmax')# Inserire softmax 3 neuroni\n",
    "])\n",
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "loss: 0.2220 - val_accuracy: 0.9117\n",
      "Epoch 360/500\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.2370 - accuracy: 0.8975 - val_loss: 0.2206 - val_accuracy: 0.9117\n",
      "Epoch 361/500\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.2428 - accuracy: 0.8971 - val_loss: 0.2194 - val_accuracy: 0.9142\n",
      "Epoch 362/500\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.2335 - accuracy: 0.9004 - val_loss: 0.2175 - val_accuracy: 0.9181\n",
      "Epoch 363/500\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.2418 - accuracy: 0.8955 - val_loss: 0.2395 - val_accuracy: 0.9065\n",
      "Epoch 364/500\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.2510 - accuracy: 0.8939 - val_loss: 0.3059 - val_accuracy: 0.8784\n",
      "Epoch 365/500\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.3135 - accuracy: 0.8795 - val_loss: 0.2377 - val_accuracy: 0.9091\n",
      "Epoch 366/500\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.2673 - accuracy: 0.8818 - val_loss: 0.2346 - val_accuracy: 0.9052\n",
      "Epoch 367/500\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.2488 - accuracy: 0.9026 - val_loss: 0.2362 - val_accuracy: 0.8963\n",
      "Epoch 368/500\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.2539 - accuracy: 0.8936 - val_loss: 0.2263 - val_accuracy: 0.9014\n",
      "Epoch 369/500\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.2378 - accuracy: 0.8997 - val_loss: 0.2189 - val_accuracy: 0.9129\n",
      "Epoch 370/500\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.2381 - accuracy: 0.8981 - val_loss: 0.2234 - val_accuracy: 0.9040\n",
      "Epoch 371/500\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.2348 - accuracy: 0.9026 - val_loss: 0.2224 - val_accuracy: 0.9091\n",
      "Epoch 372/500\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 0.2552 - accuracy: 0.8904 - val_loss: 0.2259 - val_accuracy: 0.9052\n",
      "Epoch 373/500\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.2475 - accuracy: 0.8939 - val_loss: 0.2285 - val_accuracy: 0.9155\n",
      "Epoch 374/500\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.2364 - accuracy: 0.8991 - val_loss: 0.2322 - val_accuracy: 0.9040\n",
      "Epoch 375/500\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.2393 - accuracy: 0.8965 - val_loss: 0.2257 - val_accuracy: 0.9117\n",
      "Epoch 376/500\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.2461 - accuracy: 0.8936 - val_loss: 0.2259 - val_accuracy: 0.9129\n",
      "Epoch 377/500\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.2352 - accuracy: 0.8962 - val_loss: 0.2282 - val_accuracy: 0.9027\n",
      "Epoch 378/500\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 0.2518 - accuracy: 0.8879 - val_loss: 0.2229 - val_accuracy: 0.9052\n",
      "Epoch 379/500\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.2373 - accuracy: 0.8965 - val_loss: 0.2400 - val_accuracy: 0.8912\n",
      "Epoch 380/500\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.2328 - accuracy: 0.8994 - val_loss: 0.2301 - val_accuracy: 0.9065\n",
      "Epoch 381/500\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.2403 - accuracy: 0.8959 - val_loss: 0.2499 - val_accuracy: 0.8950\n",
      "Epoch 382/500\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 0.2576 - accuracy: 0.8936 - val_loss: 0.2478 - val_accuracy: 0.9040\n",
      "Epoch 383/500\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.2434 - accuracy: 0.8955 - val_loss: 0.2325 - val_accuracy: 0.9027\n",
      "Epoch 384/500\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.2472 - accuracy: 0.8936 - val_loss: 0.2193 - val_accuracy: 0.9065\n",
      "Epoch 385/500\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.2334 - accuracy: 0.9000 - val_loss: 0.2293 - val_accuracy: 0.9091\n",
      "Epoch 386/500\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.2409 - accuracy: 0.9010 - val_loss: 0.2204 - val_accuracy: 0.9091\n",
      "Epoch 387/500\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.2502 - accuracy: 0.8943 - val_loss: 0.2180 - val_accuracy: 0.9129\n",
      "Epoch 388/500\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.2439 - accuracy: 0.8991 - val_loss: 0.2332 - val_accuracy: 0.9001\n",
      "Epoch 389/500\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.2424 - accuracy: 0.8952 - val_loss: 0.2717 - val_accuracy: 0.8873\n",
      "Epoch 390/500\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.2522 - accuracy: 0.8984 - val_loss: 0.2247 - val_accuracy: 0.8950\n",
      "Epoch 391/500\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.2539 - accuracy: 0.8952 - val_loss: 0.2305 - val_accuracy: 0.9052\n",
      "Epoch 392/500\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.2559 - accuracy: 0.8859 - val_loss: 0.2232 - val_accuracy: 0.9091\n",
      "Epoch 393/500\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.2504 - accuracy: 0.8933 - val_loss: 0.2204 - val_accuracy: 0.9142\n",
      "Epoch 394/500\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.2432 - accuracy: 0.8978 - val_loss: 0.2237 - val_accuracy: 0.9117\n",
      "Epoch 395/500\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 0.2319 - accuracy: 0.9010 - val_loss: 0.2723 - val_accuracy: 0.8720\n",
      "Epoch 396/500\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.2598 - accuracy: 0.8907 - val_loss: 0.2330 - val_accuracy: 0.9104\n",
      "Epoch 397/500\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.2465 - accuracy: 0.8939 - val_loss: 0.2215 - val_accuracy: 0.9168\n",
      "Epoch 398/500\n",
      "7/7 [==============================] - 0s 55ms/step - loss: 0.2431 - accuracy: 0.8984 - val_loss: 0.2238 - val_accuracy: 0.9142\n",
      "Epoch 399/500\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 0.2378 - accuracy: 0.8936 - val_loss: 0.2262 - val_accuracy: 0.9129\n",
      "Epoch 400/500\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.2436 - accuracy: 0.9020 - val_loss: 0.2714 - val_accuracy: 0.8720\n",
      "Epoch 401/500\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.2592 - accuracy: 0.8901 - val_loss: 0.2334 - val_accuracy: 0.8976\n",
      "Epoch 402/500\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 0.2312 - accuracy: 0.9087 - val_loss: 0.2269 - val_accuracy: 0.9155\n",
      "Epoch 403/500\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.2410 - accuracy: 0.8965 - val_loss: 0.2194 - val_accuracy: 0.9168\n",
      "Epoch 404/500\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.2515 - accuracy: 0.8920 - val_loss: 0.2272 - val_accuracy: 0.9040\n",
      "Epoch 405/500\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.2436 - accuracy: 0.8984 - val_loss: 0.2266 - val_accuracy: 0.9142\n",
      "Epoch 406/500\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.2424 - accuracy: 0.8936 - val_loss: 0.2277 - val_accuracy: 0.9129\n",
      "Epoch 407/500\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.2489 - accuracy: 0.8920 - val_loss: 0.2226 - val_accuracy: 0.9117\n",
      "Epoch 408/500\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.2457 - accuracy: 0.8955 - val_loss: 0.2236 - val_accuracy: 0.9155\n",
      "Epoch 409/500\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.2361 - accuracy: 0.9000 - val_loss: 0.2519 - val_accuracy: 0.8796\n",
      "Epoch 410/500\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.2474 - accuracy: 0.8927 - val_loss: 0.2239 - val_accuracy: 0.9117\n",
      "Epoch 411/500\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.2332 - accuracy: 0.9016 - val_loss: 0.2205 - val_accuracy: 0.9193\n",
      "Epoch 412/500\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.2352 - accuracy: 0.9004 - val_loss: 0.2186 - val_accuracy: 0.9193\n",
      "Epoch 413/500\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.2461 - accuracy: 0.8997 - val_loss: 0.2179 - val_accuracy: 0.9117\n",
      "Epoch 414/500\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.2323 - accuracy: 0.9000 - val_loss: 0.2537 - val_accuracy: 0.8937\n",
      "Epoch 415/500\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.2529 - accuracy: 0.8923 - val_loss: 0.2714 - val_accuracy: 0.8796\n",
      "Epoch 416/500\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.2600 - accuracy: 0.8920 - val_loss: 0.2369 - val_accuracy: 0.9001\n",
      "Epoch 417/500\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.2485 - accuracy: 0.8949 - val_loss: 0.2230 - val_accuracy: 0.9078\n",
      "Epoch 418/500\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.2369 - accuracy: 0.8971 - val_loss: 0.2205 - val_accuracy: 0.9155\n",
      "Epoch 419/500\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.2371 - accuracy: 0.8968 - val_loss: 0.2270 - val_accuracy: 0.9142\n",
      "Epoch 420/500\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 0.2340 - accuracy: 0.9039 - val_loss: 0.2209 - val_accuracy: 0.9168\n",
      "Epoch 421/500\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.2385 - accuracy: 0.9055 - val_loss: 0.2586 - val_accuracy: 0.8771\n",
      "Epoch 422/500\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.2404 - accuracy: 0.8988 - val_loss: 0.2268 - val_accuracy: 0.9078\n",
      "Epoch 423/500\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.2357 - accuracy: 0.9020 - val_loss: 0.2562 - val_accuracy: 0.8899\n",
      "Epoch 424/500\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.2478 - accuracy: 0.8949 - val_loss: 0.2220 - val_accuracy: 0.9052\n",
      "Epoch 425/500\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.2326 - accuracy: 0.9036 - val_loss: 0.2170 - val_accuracy: 0.9117\n",
      "Epoch 426/500\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.2331 - accuracy: 0.9042 - val_loss: 0.2360 - val_accuracy: 0.9065\n",
      "Epoch 427/500\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.2333 - accuracy: 0.9000 - val_loss: 0.2487 - val_accuracy: 0.8860\n",
      "Epoch 428/500\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.2320 - accuracy: 0.8949 - val_loss: 0.2225 - val_accuracy: 0.9181\n",
      "Epoch 429/500\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.2240 - accuracy: 0.9084 - val_loss: 0.2198 - val_accuracy: 0.9168\n",
      "Epoch 430/500\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.2296 - accuracy: 0.9077 - val_loss: 0.2208 - val_accuracy: 0.9168\n",
      "Epoch 431/500\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.2367 - accuracy: 0.9052 - val_loss: 0.2273 - val_accuracy: 0.9065\n",
      "Epoch 432/500\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.2453 - accuracy: 0.9007 - val_loss: 0.2205 - val_accuracy: 0.9142\n",
      "Epoch 433/500\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.2367 - accuracy: 0.8988 - val_loss: 0.2335 - val_accuracy: 0.9117\n",
      "Epoch 434/500\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.2438 - accuracy: 0.8984 - val_loss: 0.2347 - val_accuracy: 0.8976\n",
      "Epoch 435/500\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.2471 - accuracy: 0.8968 - val_loss: 0.2727 - val_accuracy: 0.8732\n",
      "Epoch 436/500\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.2390 - accuracy: 0.9036 - val_loss: 0.2273 - val_accuracy: 0.9142\n",
      "Epoch 437/500\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.2409 - accuracy: 0.9010 - val_loss: 0.2369 - val_accuracy: 0.9040\n",
      "Epoch 438/500\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.2320 - accuracy: 0.9071 - val_loss: 0.2278 - val_accuracy: 0.9129\n",
      "Epoch 439/500\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.2378 - accuracy: 0.9013 - val_loss: 0.2328 - val_accuracy: 0.9014\n",
      "Epoch 440/500\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.2371 - accuracy: 0.8997 - val_loss: 0.2238 - val_accuracy: 0.9155\n",
      "Epoch 441/500\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.2325 - accuracy: 0.9007 - val_loss: 0.2279 - val_accuracy: 0.9065\n",
      "Epoch 442/500\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.2662 - accuracy: 0.8882 - val_loss: 0.2285 - val_accuracy: 0.9065\n",
      "Epoch 443/500\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.2500 - accuracy: 0.8898 - val_loss: 0.2255 - val_accuracy: 0.9155\n",
      "Epoch 444/500\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.2407 - accuracy: 0.9023 - val_loss: 0.2377 - val_accuracy: 0.9091\n",
      "Epoch 445/500\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.2335 - accuracy: 0.8978 - val_loss: 0.2482 - val_accuracy: 0.8950\n",
      "Epoch 446/500\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.2518 - accuracy: 0.8933 - val_loss: 0.2385 - val_accuracy: 0.8976\n",
      "Epoch 447/500\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.2369 - accuracy: 0.8975 - val_loss: 0.2275 - val_accuracy: 0.9129\n",
      "Epoch 448/500\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.2344 - accuracy: 0.8994 - val_loss: 0.2265 - val_accuracy: 0.9027\n",
      "Epoch 449/500\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.2360 - accuracy: 0.9004 - val_loss: 0.2180 - val_accuracy: 0.9168\n",
      "Epoch 450/500\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.2373 - accuracy: 0.8978 - val_loss: 0.2232 - val_accuracy: 0.9001\n",
      "Epoch 451/500\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.2344 - accuracy: 0.8962 - val_loss: 0.2153 - val_accuracy: 0.9142\n",
      "Epoch 452/500\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.2212 - accuracy: 0.9052 - val_loss: 0.2214 - val_accuracy: 0.9027\n",
      "Epoch 453/500\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.2361 - accuracy: 0.8984 - val_loss: 0.2169 - val_accuracy: 0.9129\n",
      "Epoch 454/500\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.2397 - accuracy: 0.8984 - val_loss: 0.2175 - val_accuracy: 0.9091\n",
      "Epoch 455/500\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.2290 - accuracy: 0.9048 - val_loss: 0.2239 - val_accuracy: 0.9155\n",
      "Epoch 456/500\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.2342 - accuracy: 0.9016 - val_loss: 0.2277 - val_accuracy: 0.9104\n",
      "Epoch 457/500\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.2426 - accuracy: 0.8949 - val_loss: 0.2236 - val_accuracy: 0.9129\n",
      "Epoch 458/500\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.2434 - accuracy: 0.9007 - val_loss: 0.2325 - val_accuracy: 0.9040\n",
      "Epoch 459/500\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.2436 - accuracy: 0.9000 - val_loss: 0.2252 - val_accuracy: 0.9040\n",
      "Epoch 460/500\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.2315 - accuracy: 0.9032 - val_loss: 0.2198 - val_accuracy: 0.9168\n",
      "Epoch 461/500\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.2277 - accuracy: 0.9023 - val_loss: 0.2254 - val_accuracy: 0.9065\n",
      "Epoch 462/500\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.2311 - accuracy: 0.8971 - val_loss: 0.2323 - val_accuracy: 0.9078\n",
      "Epoch 463/500\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.2455 - accuracy: 0.8994 - val_loss: 0.2785 - val_accuracy: 0.8784\n",
      "Epoch 464/500\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.2386 - accuracy: 0.8949 - val_loss: 0.2954 - val_accuracy: 0.8745\n",
      "Epoch 465/500\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.2578 - accuracy: 0.8904 - val_loss: 0.3076 - val_accuracy: 0.8681\n",
      "Epoch 466/500\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.2613 - accuracy: 0.8911 - val_loss: 0.2257 - val_accuracy: 0.9155\n",
      "Epoch 467/500\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.2400 - accuracy: 0.9039 - val_loss: 0.2282 - val_accuracy: 0.9142\n",
      "Epoch 468/500\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.2432 - accuracy: 0.8920 - val_loss: 0.2259 - val_accuracy: 0.9129\n",
      "Epoch 469/500\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.2349 - accuracy: 0.9010 - val_loss: 0.2561 - val_accuracy: 0.8860\n",
      "Epoch 470/500\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.2392 - accuracy: 0.8991 - val_loss: 0.2426 - val_accuracy: 0.8976\n",
      "Epoch 471/500\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.2416 - accuracy: 0.8933 - val_loss: 0.2302 - val_accuracy: 0.9104\n",
      "Epoch 472/500\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 0.2401 - accuracy: 0.8939 - val_loss: 0.2338 - val_accuracy: 0.8950\n",
      "Epoch 473/500\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 0.2623 - accuracy: 0.8866 - val_loss: 0.2264 - val_accuracy: 0.9040\n",
      "Epoch 474/500\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.2425 - accuracy: 0.8997 - val_loss: 0.2330 - val_accuracy: 0.8976\n",
      "Epoch 475/500\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.2395 - accuracy: 0.9007 - val_loss: 0.2197 - val_accuracy: 0.9078\n",
      "Epoch 476/500\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.2418 - accuracy: 0.8943 - val_loss: 0.2356 - val_accuracy: 0.8963\n",
      "Epoch 477/500\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 0.2407 - accuracy: 0.8978 - val_loss: 0.2276 - val_accuracy: 0.9052\n",
      "Epoch 478/500\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 0.2407 - accuracy: 0.9010 - val_loss: 0.2263 - val_accuracy: 0.9078\n",
      "Epoch 479/500\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.2357 - accuracy: 0.8981 - val_loss: 0.2267 - val_accuracy: 0.9001\n",
      "Epoch 480/500\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 0.2426 - accuracy: 0.8975 - val_loss: 0.2290 - val_accuracy: 0.9001\n",
      "Epoch 481/500\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.2325 - accuracy: 0.9013 - val_loss: 0.2422 - val_accuracy: 0.8963\n",
      "Epoch 482/500\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.2390 - accuracy: 0.8943 - val_loss: 0.2251 - val_accuracy: 0.9040\n",
      "Epoch 483/500\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.2303 - accuracy: 0.8962 - val_loss: 0.2190 - val_accuracy: 0.9155\n",
      "Epoch 484/500\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.2281 - accuracy: 0.9052 - val_loss: 0.2206 - val_accuracy: 0.9117\n",
      "Epoch 485/500\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.2363 - accuracy: 0.8971 - val_loss: 0.2171 - val_accuracy: 0.9078\n",
      "Epoch 486/500\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.2290 - accuracy: 0.9029 - val_loss: 0.2251 - val_accuracy: 0.9040\n",
      "Epoch 487/500\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 0.2256 - accuracy: 0.9029 - val_loss: 0.2444 - val_accuracy: 0.8937\n",
      "Epoch 488/500\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.2292 - accuracy: 0.9026 - val_loss: 0.2277 - val_accuracy: 0.8976\n",
      "Epoch 489/500\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.2376 - accuracy: 0.9026 - val_loss: 0.2107 - val_accuracy: 0.9168\n",
      "Epoch 490/500\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.2370 - accuracy: 0.9010 - val_loss: 0.2278 - val_accuracy: 0.9052\n",
      "Epoch 491/500\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.2320 - accuracy: 0.9061 - val_loss: 0.2347 - val_accuracy: 0.9065\n",
      "Epoch 492/500\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.2356 - accuracy: 0.8975 - val_loss: 0.2180 - val_accuracy: 0.9129\n",
      "Epoch 493/500\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.2368 - accuracy: 0.9013 - val_loss: 0.2206 - val_accuracy: 0.9155\n",
      "Epoch 494/500\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.2314 - accuracy: 0.8997 - val_loss: 0.2239 - val_accuracy: 0.9104\n",
      "Epoch 495/500\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.2274 - accuracy: 0.9013 - val_loss: 0.2100 - val_accuracy: 0.9181\n",
      "Epoch 496/500\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 0.2295 - accuracy: 0.9023 - val_loss: 0.2376 - val_accuracy: 0.8899\n",
      "Epoch 497/500\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.2349 - accuracy: 0.9048 - val_loss: 0.2287 - val_accuracy: 0.9065\n",
      "Epoch 498/500\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.2461 - accuracy: 0.8955 - val_loss: 0.2242 - val_accuracy: 0.9052\n",
      "Epoch 499/500\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.2278 - accuracy: 0.8994 - val_loss: 0.2184 - val_accuracy: 0.9104\n",
      "Epoch 500/500\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.2264 - accuracy: 0.9007 - val_loss: 0.2125 - val_accuracy: 0.9117\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_set, train_label,\n",
    "                    epochs=500,\n",
    "                    batch_size=512,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"277.314375pt\" version=\"1.1\" viewBox=\"0 0 385.78125 277.314375\" width=\"385.78125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <defs>\r\n  <style type=\"text/css\">\r\n*{stroke-linecap:butt;stroke-linejoin:round;}\r\n  </style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 277.314375 \r\nL 385.78125 277.314375 \r\nL 385.78125 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 43.78125 239.758125 \r\nL 378.58125 239.758125 \r\nL 378.58125 22.318125 \r\nL 43.78125 22.318125 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"m7dce3b66f3\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"58.389485\" xlink:href=\"#m7dce3b66f3\" y=\"239.758125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0 -->\r\n      <defs>\r\n       <path d=\"M 31.78125 66.40625 \r\nQ 24.171875 66.40625 20.328125 58.90625 \r\nQ 16.5 51.421875 16.5 36.375 \r\nQ 16.5 21.390625 20.328125 13.890625 \r\nQ 24.171875 6.390625 31.78125 6.390625 \r\nQ 39.453125 6.390625 43.28125 13.890625 \r\nQ 47.125 21.390625 47.125 36.375 \r\nQ 47.125 51.421875 43.28125 58.90625 \r\nQ 39.453125 66.40625 31.78125 66.40625 \r\nz\r\nM 31.78125 74.21875 \r\nQ 44.046875 74.21875 50.515625 64.515625 \r\nQ 56.984375 54.828125 56.984375 36.375 \r\nQ 56.984375 17.96875 50.515625 8.265625 \r\nQ 44.046875 -1.421875 31.78125 -1.421875 \r\nQ 19.53125 -1.421875 13.0625 8.265625 \r\nQ 6.59375 17.96875 6.59375 36.375 \r\nQ 6.59375 54.828125 13.0625 64.515625 \r\nQ 19.53125 74.21875 31.78125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-48\"/>\r\n      </defs>\r\n      <g transform=\"translate(55.208235 254.356562)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"119.384201\" xlink:href=\"#m7dce3b66f3\" y=\"239.758125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 100 -->\r\n      <defs>\r\n       <path d=\"M 12.40625 8.296875 \r\nL 28.515625 8.296875 \r\nL 28.515625 63.921875 \r\nL 10.984375 60.40625 \r\nL 10.984375 69.390625 \r\nL 28.421875 72.90625 \r\nL 38.28125 72.90625 \r\nL 38.28125 8.296875 \r\nL 54.390625 8.296875 \r\nL 54.390625 0 \r\nL 12.40625 0 \r\nz\r\n\" id=\"DejaVuSans-49\"/>\r\n      </defs>\r\n      <g transform=\"translate(109.840451 254.356562)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"180.378918\" xlink:href=\"#m7dce3b66f3\" y=\"239.758125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 200 -->\r\n      <defs>\r\n       <path d=\"M 19.1875 8.296875 \r\nL 53.609375 8.296875 \r\nL 53.609375 0 \r\nL 7.328125 0 \r\nL 7.328125 8.296875 \r\nQ 12.9375 14.109375 22.625 23.890625 \r\nQ 32.328125 33.6875 34.8125 36.53125 \r\nQ 39.546875 41.84375 41.421875 45.53125 \r\nQ 43.3125 49.21875 43.3125 52.78125 \r\nQ 43.3125 58.59375 39.234375 62.25 \r\nQ 35.15625 65.921875 28.609375 65.921875 \r\nQ 23.96875 65.921875 18.8125 64.3125 \r\nQ 13.671875 62.703125 7.8125 59.421875 \r\nL 7.8125 69.390625 \r\nQ 13.765625 71.78125 18.9375 73 \r\nQ 24.125 74.21875 28.421875 74.21875 \r\nQ 39.75 74.21875 46.484375 68.546875 \r\nQ 53.21875 62.890625 53.21875 53.421875 \r\nQ 53.21875 48.921875 51.53125 44.890625 \r\nQ 49.859375 40.875 45.40625 35.40625 \r\nQ 44.1875 33.984375 37.640625 27.21875 \r\nQ 31.109375 20.453125 19.1875 8.296875 \r\nz\r\n\" id=\"DejaVuSans-50\"/>\r\n      </defs>\r\n      <g transform=\"translate(170.835168 254.356562)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"241.373635\" xlink:href=\"#m7dce3b66f3\" y=\"239.758125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 300 -->\r\n      <defs>\r\n       <path d=\"M 40.578125 39.3125 \r\nQ 47.65625 37.796875 51.625 33 \r\nQ 55.609375 28.21875 55.609375 21.1875 \r\nQ 55.609375 10.40625 48.1875 4.484375 \r\nQ 40.765625 -1.421875 27.09375 -1.421875 \r\nQ 22.515625 -1.421875 17.65625 -0.515625 \r\nQ 12.796875 0.390625 7.625 2.203125 \r\nL 7.625 11.71875 \r\nQ 11.71875 9.328125 16.59375 8.109375 \r\nQ 21.484375 6.890625 26.8125 6.890625 \r\nQ 36.078125 6.890625 40.9375 10.546875 \r\nQ 45.796875 14.203125 45.796875 21.1875 \r\nQ 45.796875 27.640625 41.28125 31.265625 \r\nQ 36.765625 34.90625 28.71875 34.90625 \r\nL 20.21875 34.90625 \r\nL 20.21875 43.015625 \r\nL 29.109375 43.015625 \r\nQ 36.375 43.015625 40.234375 45.921875 \r\nQ 44.09375 48.828125 44.09375 54.296875 \r\nQ 44.09375 59.90625 40.109375 62.90625 \r\nQ 36.140625 65.921875 28.71875 65.921875 \r\nQ 24.65625 65.921875 20.015625 65.03125 \r\nQ 15.375 64.15625 9.8125 62.3125 \r\nL 9.8125 71.09375 \r\nQ 15.4375 72.65625 20.34375 73.4375 \r\nQ 25.25 74.21875 29.59375 74.21875 \r\nQ 40.828125 74.21875 47.359375 69.109375 \r\nQ 53.90625 64.015625 53.90625 55.328125 \r\nQ 53.90625 49.265625 50.4375 45.09375 \r\nQ 46.96875 40.921875 40.578125 39.3125 \r\nz\r\n\" id=\"DejaVuSans-51\"/>\r\n      </defs>\r\n      <g transform=\"translate(231.829885 254.356562)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-51\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"302.368351\" xlink:href=\"#m7dce3b66f3\" y=\"239.758125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 400 -->\r\n      <defs>\r\n       <path d=\"M 37.796875 64.3125 \r\nL 12.890625 25.390625 \r\nL 37.796875 25.390625 \r\nz\r\nM 35.203125 72.90625 \r\nL 47.609375 72.90625 \r\nL 47.609375 25.390625 \r\nL 58.015625 25.390625 \r\nL 58.015625 17.1875 \r\nL 47.609375 17.1875 \r\nL 47.609375 0 \r\nL 37.796875 0 \r\nL 37.796875 17.1875 \r\nL 4.890625 17.1875 \r\nL 4.890625 26.703125 \r\nz\r\n\" id=\"DejaVuSans-52\"/>\r\n      </defs>\r\n      <g transform=\"translate(292.824601 254.356562)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-52\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"363.363068\" xlink:href=\"#m7dce3b66f3\" y=\"239.758125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 500 -->\r\n      <defs>\r\n       <path d=\"M 10.796875 72.90625 \r\nL 49.515625 72.90625 \r\nL 49.515625 64.59375 \r\nL 19.828125 64.59375 \r\nL 19.828125 46.734375 \r\nQ 21.96875 47.46875 24.109375 47.828125 \r\nQ 26.265625 48.1875 28.421875 48.1875 \r\nQ 40.625 48.1875 47.75 41.5 \r\nQ 54.890625 34.8125 54.890625 23.390625 \r\nQ 54.890625 11.625 47.5625 5.09375 \r\nQ 40.234375 -1.421875 26.90625 -1.421875 \r\nQ 22.3125 -1.421875 17.546875 -0.640625 \r\nQ 12.796875 0.140625 7.71875 1.703125 \r\nL 7.71875 11.625 \r\nQ 12.109375 9.234375 16.796875 8.0625 \r\nQ 21.484375 6.890625 26.703125 6.890625 \r\nQ 35.15625 6.890625 40.078125 11.328125 \r\nQ 45.015625 15.765625 45.015625 23.390625 \r\nQ 45.015625 31 40.078125 35.4375 \r\nQ 35.15625 39.890625 26.703125 39.890625 \r\nQ 22.75 39.890625 18.8125 39.015625 \r\nQ 14.890625 38.140625 10.796875 36.28125 \r\nz\r\n\" id=\"DejaVuSans-53\"/>\r\n      </defs>\r\n      <g transform=\"translate(353.819318 254.356562)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"text_7\">\r\n     <!-- Epochs -->\r\n     <defs>\r\n      <path d=\"M 9.8125 72.90625 \r\nL 55.90625 72.90625 \r\nL 55.90625 64.59375 \r\nL 19.671875 64.59375 \r\nL 19.671875 43.015625 \r\nL 54.390625 43.015625 \r\nL 54.390625 34.71875 \r\nL 19.671875 34.71875 \r\nL 19.671875 8.296875 \r\nL 56.78125 8.296875 \r\nL 56.78125 0 \r\nL 9.8125 0 \r\nz\r\n\" id=\"DejaVuSans-69\"/>\r\n      <path d=\"M 18.109375 8.203125 \r\nL 18.109375 -20.796875 \r\nL 9.078125 -20.796875 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.390625 \r\nQ 20.953125 51.265625 25.265625 53.625 \r\nQ 29.59375 56 35.59375 56 \r\nQ 45.5625 56 51.78125 48.09375 \r\nQ 58.015625 40.1875 58.015625 27.296875 \r\nQ 58.015625 14.40625 51.78125 6.484375 \r\nQ 45.5625 -1.421875 35.59375 -1.421875 \r\nQ 29.59375 -1.421875 25.265625 0.953125 \r\nQ 20.953125 3.328125 18.109375 8.203125 \r\nz\r\nM 48.6875 27.296875 \r\nQ 48.6875 37.203125 44.609375 42.84375 \r\nQ 40.53125 48.484375 33.40625 48.484375 \r\nQ 26.265625 48.484375 22.1875 42.84375 \r\nQ 18.109375 37.203125 18.109375 27.296875 \r\nQ 18.109375 17.390625 22.1875 11.75 \r\nQ 26.265625 6.109375 33.40625 6.109375 \r\nQ 40.53125 6.109375 44.609375 11.75 \r\nQ 48.6875 17.390625 48.6875 27.296875 \r\nz\r\n\" id=\"DejaVuSans-112\"/>\r\n      <path d=\"M 30.609375 48.390625 \r\nQ 23.390625 48.390625 19.1875 42.75 \r\nQ 14.984375 37.109375 14.984375 27.296875 \r\nQ 14.984375 17.484375 19.15625 11.84375 \r\nQ 23.34375 6.203125 30.609375 6.203125 \r\nQ 37.796875 6.203125 41.984375 11.859375 \r\nQ 46.1875 17.53125 46.1875 27.296875 \r\nQ 46.1875 37.015625 41.984375 42.703125 \r\nQ 37.796875 48.390625 30.609375 48.390625 \r\nz\r\nM 30.609375 56 \r\nQ 42.328125 56 49.015625 48.375 \r\nQ 55.71875 40.765625 55.71875 27.296875 \r\nQ 55.71875 13.875 49.015625 6.21875 \r\nQ 42.328125 -1.421875 30.609375 -1.421875 \r\nQ 18.84375 -1.421875 12.171875 6.21875 \r\nQ 5.515625 13.875 5.515625 27.296875 \r\nQ 5.515625 40.765625 12.171875 48.375 \r\nQ 18.84375 56 30.609375 56 \r\nz\r\n\" id=\"DejaVuSans-111\"/>\r\n      <path d=\"M 48.78125 52.59375 \r\nL 48.78125 44.1875 \r\nQ 44.96875 46.296875 41.140625 47.34375 \r\nQ 37.3125 48.390625 33.40625 48.390625 \r\nQ 24.65625 48.390625 19.8125 42.84375 \r\nQ 14.984375 37.3125 14.984375 27.296875 \r\nQ 14.984375 17.28125 19.8125 11.734375 \r\nQ 24.65625 6.203125 33.40625 6.203125 \r\nQ 37.3125 6.203125 41.140625 7.25 \r\nQ 44.96875 8.296875 48.78125 10.40625 \r\nL 48.78125 2.09375 \r\nQ 45.015625 0.34375 40.984375 -0.53125 \r\nQ 36.96875 -1.421875 32.421875 -1.421875 \r\nQ 20.0625 -1.421875 12.78125 6.34375 \r\nQ 5.515625 14.109375 5.515625 27.296875 \r\nQ 5.515625 40.671875 12.859375 48.328125 \r\nQ 20.21875 56 33.015625 56 \r\nQ 37.15625 56 41.109375 55.140625 \r\nQ 45.0625 54.296875 48.78125 52.59375 \r\nz\r\n\" id=\"DejaVuSans-99\"/>\r\n      <path d=\"M 54.890625 33.015625 \r\nL 54.890625 0 \r\nL 45.90625 0 \r\nL 45.90625 32.71875 \r\nQ 45.90625 40.484375 42.875 44.328125 \r\nQ 39.84375 48.1875 33.796875 48.1875 \r\nQ 26.515625 48.1875 22.3125 43.546875 \r\nQ 18.109375 38.921875 18.109375 30.90625 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 75.984375 \r\nL 18.109375 75.984375 \r\nL 18.109375 46.1875 \r\nQ 21.34375 51.125 25.703125 53.5625 \r\nQ 30.078125 56 35.796875 56 \r\nQ 45.21875 56 50.046875 50.171875 \r\nQ 54.890625 44.34375 54.890625 33.015625 \r\nz\r\n\" id=\"DejaVuSans-104\"/>\r\n      <path d=\"M 44.28125 53.078125 \r\nL 44.28125 44.578125 \r\nQ 40.484375 46.53125 36.375 47.5 \r\nQ 32.28125 48.484375 27.875 48.484375 \r\nQ 21.1875 48.484375 17.84375 46.4375 \r\nQ 14.5 44.390625 14.5 40.28125 \r\nQ 14.5 37.15625 16.890625 35.375 \r\nQ 19.28125 33.59375 26.515625 31.984375 \r\nL 29.59375 31.296875 \r\nQ 39.15625 29.25 43.1875 25.515625 \r\nQ 47.21875 21.78125 47.21875 15.09375 \r\nQ 47.21875 7.46875 41.1875 3.015625 \r\nQ 35.15625 -1.421875 24.609375 -1.421875 \r\nQ 20.21875 -1.421875 15.453125 -0.5625 \r\nQ 10.6875 0.296875 5.421875 2 \r\nL 5.421875 11.28125 \r\nQ 10.40625 8.6875 15.234375 7.390625 \r\nQ 20.0625 6.109375 24.8125 6.109375 \r\nQ 31.15625 6.109375 34.5625 8.28125 \r\nQ 37.984375 10.453125 37.984375 14.40625 \r\nQ 37.984375 18.0625 35.515625 20.015625 \r\nQ 33.0625 21.96875 24.703125 23.78125 \r\nL 21.578125 24.515625 \r\nQ 13.234375 26.265625 9.515625 29.90625 \r\nQ 5.8125 33.546875 5.8125 39.890625 \r\nQ 5.8125 47.609375 11.28125 51.796875 \r\nQ 16.75 56 26.8125 56 \r\nQ 31.78125 56 36.171875 55.265625 \r\nQ 40.578125 54.546875 44.28125 53.078125 \r\nz\r\n\" id=\"DejaVuSans-115\"/>\r\n     </defs>\r\n     <g transform=\"translate(193.265625 268.034687)scale(0.1 -0.1)\">\r\n      <use xlink:href=\"#DejaVuSans-69\"/>\r\n      <use x=\"63.183594\" xlink:href=\"#DejaVuSans-112\"/>\r\n      <use x=\"126.660156\" xlink:href=\"#DejaVuSans-111\"/>\r\n      <use x=\"187.841797\" xlink:href=\"#DejaVuSans-99\"/>\r\n      <use x=\"242.822266\" xlink:href=\"#DejaVuSans-104\"/>\r\n      <use x=\"306.201172\" xlink:href=\"#DejaVuSans-115\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_7\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"mf15a1d3fe9\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#mf15a1d3fe9\" y=\"237.120991\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 0.4 -->\r\n      <defs>\r\n       <path d=\"M 10.6875 12.40625 \r\nL 21 12.40625 \r\nL 21 0 \r\nL 10.6875 0 \r\nz\r\n\" id=\"DejaVuSans-46\"/>\r\n      </defs>\r\n      <g transform=\"translate(20.878125 240.92021)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_8\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#mf15a1d3fe9\" y=\"197.856538\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 0.5 -->\r\n      <g transform=\"translate(20.878125 201.655756)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#mf15a1d3fe9\" y=\"158.592084\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 0.6 -->\r\n      <defs>\r\n       <path d=\"M 33.015625 40.375 \r\nQ 26.375 40.375 22.484375 35.828125 \r\nQ 18.609375 31.296875 18.609375 23.390625 \r\nQ 18.609375 15.53125 22.484375 10.953125 \r\nQ 26.375 6.390625 33.015625 6.390625 \r\nQ 39.65625 6.390625 43.53125 10.953125 \r\nQ 47.40625 15.53125 47.40625 23.390625 \r\nQ 47.40625 31.296875 43.53125 35.828125 \r\nQ 39.65625 40.375 33.015625 40.375 \r\nz\r\nM 52.59375 71.296875 \r\nL 52.59375 62.3125 \r\nQ 48.875 64.0625 45.09375 64.984375 \r\nQ 41.3125 65.921875 37.59375 65.921875 \r\nQ 27.828125 65.921875 22.671875 59.328125 \r\nQ 17.53125 52.734375 16.796875 39.40625 \r\nQ 19.671875 43.65625 24.015625 45.921875 \r\nQ 28.375 48.1875 33.59375 48.1875 \r\nQ 44.578125 48.1875 50.953125 41.515625 \r\nQ 57.328125 34.859375 57.328125 23.390625 \r\nQ 57.328125 12.15625 50.6875 5.359375 \r\nQ 44.046875 -1.421875 33.015625 -1.421875 \r\nQ 20.359375 -1.421875 13.671875 8.265625 \r\nQ 6.984375 17.96875 6.984375 36.375 \r\nQ 6.984375 53.65625 15.1875 63.9375 \r\nQ 23.390625 74.21875 37.203125 74.21875 \r\nQ 40.921875 74.21875 44.703125 73.484375 \r\nQ 48.484375 72.75 52.59375 71.296875 \r\nz\r\n\" id=\"DejaVuSans-54\"/>\r\n      </defs>\r\n      <g transform=\"translate(20.878125 162.391303)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-54\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#mf15a1d3fe9\" y=\"119.327631\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 0.7 -->\r\n      <defs>\r\n       <path d=\"M 8.203125 72.90625 \r\nL 55.078125 72.90625 \r\nL 55.078125 68.703125 \r\nL 28.609375 0 \r\nL 18.3125 0 \r\nL 43.21875 64.59375 \r\nL 8.203125 64.59375 \r\nz\r\n\" id=\"DejaVuSans-55\"/>\r\n      </defs>\r\n      <g transform=\"translate(20.878125 123.12685)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-55\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#mf15a1d3fe9\" y=\"80.063177\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- 0.8 -->\r\n      <defs>\r\n       <path d=\"M 31.78125 34.625 \r\nQ 24.75 34.625 20.71875 30.859375 \r\nQ 16.703125 27.09375 16.703125 20.515625 \r\nQ 16.703125 13.921875 20.71875 10.15625 \r\nQ 24.75 6.390625 31.78125 6.390625 \r\nQ 38.8125 6.390625 42.859375 10.171875 \r\nQ 46.921875 13.96875 46.921875 20.515625 \r\nQ 46.921875 27.09375 42.890625 30.859375 \r\nQ 38.875 34.625 31.78125 34.625 \r\nz\r\nM 21.921875 38.8125 \r\nQ 15.578125 40.375 12.03125 44.71875 \r\nQ 8.5 49.078125 8.5 55.328125 \r\nQ 8.5 64.0625 14.71875 69.140625 \r\nQ 20.953125 74.21875 31.78125 74.21875 \r\nQ 42.671875 74.21875 48.875 69.140625 \r\nQ 55.078125 64.0625 55.078125 55.328125 \r\nQ 55.078125 49.078125 51.53125 44.71875 \r\nQ 48 40.375 41.703125 38.8125 \r\nQ 48.828125 37.15625 52.796875 32.3125 \r\nQ 56.78125 27.484375 56.78125 20.515625 \r\nQ 56.78125 9.90625 50.3125 4.234375 \r\nQ 43.84375 -1.421875 31.78125 -1.421875 \r\nQ 19.734375 -1.421875 13.25 4.234375 \r\nQ 6.78125 9.90625 6.78125 20.515625 \r\nQ 6.78125 27.484375 10.78125 32.3125 \r\nQ 14.796875 37.15625 21.921875 38.8125 \r\nz\r\nM 18.3125 54.390625 \r\nQ 18.3125 48.734375 21.84375 45.5625 \r\nQ 25.390625 42.390625 31.78125 42.390625 \r\nQ 38.140625 42.390625 41.71875 45.5625 \r\nQ 45.3125 48.734375 45.3125 54.390625 \r\nQ 45.3125 60.0625 41.71875 63.234375 \r\nQ 38.140625 66.40625 31.78125 66.40625 \r\nQ 25.390625 66.40625 21.84375 63.234375 \r\nQ 18.3125 60.0625 18.3125 54.390625 \r\nz\r\n\" id=\"DejaVuSans-56\"/>\r\n      </defs>\r\n      <g transform=\"translate(20.878125 83.862396)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-56\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_6\">\r\n     <g id=\"line2d_12\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"43.78125\" xlink:href=\"#mf15a1d3fe9\" y=\"40.798724\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_13\">\r\n      <!-- 0.9 -->\r\n      <defs>\r\n       <path d=\"M 10.984375 1.515625 \r\nL 10.984375 10.5 \r\nQ 14.703125 8.734375 18.5 7.8125 \r\nQ 22.3125 6.890625 25.984375 6.890625 \r\nQ 35.75 6.890625 40.890625 13.453125 \r\nQ 46.046875 20.015625 46.78125 33.40625 \r\nQ 43.953125 29.203125 39.59375 26.953125 \r\nQ 35.25 24.703125 29.984375 24.703125 \r\nQ 19.046875 24.703125 12.671875 31.3125 \r\nQ 6.296875 37.9375 6.296875 49.421875 \r\nQ 6.296875 60.640625 12.9375 67.421875 \r\nQ 19.578125 74.21875 30.609375 74.21875 \r\nQ 43.265625 74.21875 49.921875 64.515625 \r\nQ 56.59375 54.828125 56.59375 36.375 \r\nQ 56.59375 19.140625 48.40625 8.859375 \r\nQ 40.234375 -1.421875 26.421875 -1.421875 \r\nQ 22.703125 -1.421875 18.890625 -0.6875 \r\nQ 15.09375 0.046875 10.984375 1.515625 \r\nz\r\nM 30.609375 32.421875 \r\nQ 37.25 32.421875 41.125 36.953125 \r\nQ 45.015625 41.5 45.015625 49.421875 \r\nQ 45.015625 57.28125 41.125 61.84375 \r\nQ 37.25 66.40625 30.609375 66.40625 \r\nQ 23.96875 66.40625 20.09375 61.84375 \r\nQ 16.21875 57.28125 16.21875 49.421875 \r\nQ 16.21875 41.5 20.09375 36.953125 \r\nQ 23.96875 32.421875 30.609375 32.421875 \r\nz\r\n\" id=\"DejaVuSans-57\"/>\r\n      </defs>\r\n      <g transform=\"translate(20.878125 44.597943)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-57\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"text_14\">\r\n     <!-- Acc -->\r\n     <defs>\r\n      <path d=\"M 34.1875 63.1875 \r\nL 20.796875 26.90625 \r\nL 47.609375 26.90625 \r\nz\r\nM 28.609375 72.90625 \r\nL 39.796875 72.90625 \r\nL 67.578125 0 \r\nL 57.328125 0 \r\nL 50.6875 18.703125 \r\nL 17.828125 18.703125 \r\nL 11.1875 0 \r\nL 0.78125 0 \r\nz\r\n\" id=\"DejaVuSans-65\"/>\r\n     </defs>\r\n     <g transform=\"translate(14.798438 139.956094)rotate(-90)scale(0.1 -0.1)\">\r\n      <use xlink:href=\"#DejaVuSans-65\"/>\r\n      <use x=\"68.392578\" xlink:href=\"#DejaVuSans-99\"/>\r\n      <use x=\"123.373047\" xlink:href=\"#DejaVuSans-99\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"line2d_13\">\r\n    <path clip-path=\"url(#p3bd1476cc0)\" d=\"M 58.999432 229.874489 \r\nL 59.609379 195.151685 \r\nL 60.219326 207.354987 \r\nL 61.43922 190.370995 \r\nL 62.049168 193.390389 \r\nL 62.659115 185.464511 \r\nL 63.269062 181.564492 \r\nL 63.879009 176.658008 \r\nL 64.488956 170.367655 \r\nL 65.708851 152.754626 \r\nL 66.318798 134.764192 \r\nL 66.928745 125.202835 \r\nL 67.538692 120.673778 \r\nL 68.148639 111.364031 \r\nL 68.758586 108.470453 \r\nL 69.978481 97.651044 \r\nL 70.588428 94.128428 \r\nL 71.198375 94.128428 \r\nL 71.808322 89.599372 \r\nL 72.418269 89.473555 \r\nL 73.028217 83.434813 \r\nL 73.638164 84.944498 \r\nL 74.248111 83.686423 \r\nL 75.468005 82.428348 \r\nL 76.077952 79.786404 \r\nL 76.6879 82.302554 \r\nL 77.297847 78.905756 \r\nL 77.907794 74.87992 \r\nL 78.517741 76.012178 \r\nL 79.127688 75.13153 \r\nL 79.737635 73.496028 \r\nL 80.347583 70.854084 \r\nL 80.95753 71.734732 \r\nL 82.177424 68.46375 \r\nL 83.397319 75.508957 \r\nL 84.007266 65.444379 \r\nL 84.617213 70.476657 \r\nL 85.22716 64.060487 \r\nL 85.837107 69.470215 \r\nL 86.447054 64.689525 \r\nL 87.057002 65.9476 \r\nL 87.666949 72.363769 \r\nL 88.276896 63.054022 \r\nL 88.886843 62.424985 \r\nL 89.49679 63.179839 \r\nL 90.106737 64.941135 \r\nL 90.716685 60.789506 \r\nL 91.326632 61.921764 \r\nL 91.936579 61.418543 \r\nL 92.546526 63.934694 \r\nL 93.156473 60.915299 \r\nL 93.76642 60.915299 \r\nL 94.376368 61.544337 \r\nL 94.986315 59.154003 \r\nL 95.596262 60.537895 \r\nL 96.206209 63.179839 \r\nL 96.816156 61.166933 \r\nL 97.426103 57.518501 \r\nL 98.645998 56.386242 \r\nL 99.255945 58.902393 \r\nL 99.865892 57.770134 \r\nL 100.475839 54.876557 \r\nL 101.695734 61.418543 \r\nL 102.305681 61.921764 \r\nL 102.915628 59.657247 \r\nL 103.525575 55.631411 \r\nL 104.135522 56.76367 \r\nL 104.745469 56.76367 \r\nL 105.355417 58.524965 \r\nL 105.965364 58.399172 \r\nL 106.575311 51.605575 \r\nL 107.185258 54.624946 \r\nL 107.795205 51.983002 \r\nL 108.405152 51.353965 \r\nL 109.0151 62.299191 \r\nL 109.625047 51.983002 \r\nL 110.234994 53.744298 \r\nL 110.844941 52.486223 \r\nL 111.454888 52.108796 \r\nL 112.064835 54.624946 \r\nL 112.674783 51.228148 \r\nL 113.28473 54.499129 \r\nL 113.894677 50.59911 \r\nL 114.504624 51.983002 \r\nL 115.114571 52.108796 \r\nL 115.724518 51.479758 \r\nL 116.334466 61.670153 \r\nL 116.944413 55.128167 \r\nL 117.55436 53.995909 \r\nL 118.164307 54.750763 \r\nL 118.774254 54.624946 \r\nL 119.384201 55.379777 \r\nL 119.994149 53.618481 \r\nL 120.604096 57.770134 \r\nL 121.214043 48.711998 \r\nL 121.82399 53.492688 \r\nL 122.433937 53.995909 \r\nL 123.043884 52.360406 \r\nL 123.653832 51.857186 \r\nL 124.263779 50.473293 \r\nL 124.873726 51.479758 \r\nL 125.483673 52.234613 \r\nL 126.09362 48.586204 \r\nL 126.703567 52.234613 \r\nL 127.313515 50.724927 \r\nL 127.923462 51.857186 \r\nL 128.533409 48.837814 \r\nL 129.143356 52.360406 \r\nL 129.753303 53.618481 \r\nL 130.36325 52.86365 \r\nL 130.973198 52.612017 \r\nL 131.583145 56.008815 \r\nL 132.193092 55.128167 \r\nL 132.803039 53.366871 \r\nL 133.412986 54.247519 \r\nL 134.022933 53.492688 \r\nL 134.632881 50.09589 \r\nL 135.242828 52.360406 \r\nL 135.852775 50.3475 \r\nL 136.462722 51.479758 \r\nL 137.072669 49.844279 \r\nL 137.682616 51.731369 \r\nL 138.902511 47.957166 \r\nL 139.512458 47.579739 \r\nL 140.122405 49.718462 \r\nL 140.732352 50.850721 \r\nL 141.342299 47.705533 \r\nL 141.952247 49.592645 \r\nL 142.562194 49.718462 \r\nL 143.782088 48.33457 \r\nL 144.392035 46.573274 \r\nL 145.001982 47.076518 \r\nL 145.61193 50.221683 \r\nL 146.221877 48.837814 \r\nL 146.831824 51.605575 \r\nL 147.441771 46.321664 \r\nL 148.051718 50.850721 \r\nL 148.661665 46.321664 \r\nL 149.271613 48.837814 \r\nL 149.88156 47.328129 \r\nL 150.491507 53.744298 \r\nL 151.101454 46.950702 \r\nL 151.711401 49.592645 \r\nL 152.321348 47.453922 \r\nL 152.931296 46.699091 \r\nL 153.541243 48.208777 \r\nL 154.15119 50.473293 \r\nL 154.761137 46.824885 \r\nL 155.371084 47.705533 \r\nL 155.981031 43.805514 \r\nL 156.590979 46.447481 \r\nL 157.200926 42.924866 \r\nL 157.810873 52.234613 \r\nL 158.42082 52.360406 \r\nL 159.030767 50.850721 \r\nL 159.640714 47.579739 \r\nL 160.250662 46.824885 \r\nL 160.860609 50.724927 \r\nL 161.470556 48.208777 \r\nL 162.080503 47.705533 \r\nL 162.69045 43.805514 \r\nL 163.300397 46.321664 \r\nL 163.910345 46.573274 \r\nL 164.520292 47.202312 \r\nL 165.130239 46.195847 \r\nL 165.740186 45.692626 \r\nL 166.350133 48.711998 \r\nL 166.96008 49.466852 \r\nL 167.570028 46.699091 \r\nL 168.179975 45.315199 \r\nL 168.789922 45.189406 \r\nL 169.399869 44.937795 \r\nL 170.009816 46.824885 \r\nL 170.619763 43.93133 \r\nL 171.229711 45.063589 \r\nL 171.839658 42.295828 \r\nL 172.449605 46.447481 \r\nL 173.059552 44.308758 \r\nL 173.669499 46.447481 \r\nL 174.279446 44.937795 \r\nL 174.889394 46.824885 \r\nL 175.499341 46.824885 \r\nL 176.109288 43.67972 \r\nL 176.719235 48.460387 \r\nL 177.329182 49.844279 \r\nL 177.939129 49.215242 \r\nL 178.549077 49.970073 \r\nL 179.159024 49.089425 \r\nL 179.768971 52.612017 \r\nL 180.378918 51.353965 \r\nL 180.988865 49.341035 \r\nL 181.598812 49.341035 \r\nL 182.20876 53.618481 \r\nL 182.818707 43.67972 \r\nL 183.428654 47.83135 \r\nL 184.038601 44.937795 \r\nL 184.648548 44.686162 \r\nL 185.258495 45.063589 \r\nL 185.868443 44.686162 \r\nL 187.088337 49.089425 \r\nL 187.698284 43.805514 \r\nL 188.308231 48.208777 \r\nL 188.918178 44.182941 \r\nL 189.528126 45.189406 \r\nL 190.138073 46.950702 \r\nL 190.74802 48.08296 \r\nL 191.357967 46.447481 \r\nL 191.967914 46.070054 \r\nL 192.577861 46.699091 \r\nL 193.187809 42.421645 \r\nL 193.797756 45.818443 \r\nL 194.407703 48.586204 \r\nL 195.01765 47.83135 \r\nL 195.627597 44.057124 \r\nL 196.237544 47.579739 \r\nL 196.847492 45.818443 \r\nL 197.457439 45.692626 \r\nL 198.067386 48.837814 \r\nL 198.677333 51.102331 \r\nL 199.28728 48.208777 \r\nL 199.897227 44.560368 \r\nL 200.507175 43.93133 \r\nL 201.117122 47.328129 \r\nL 201.727069 50.09589 \r\nL 202.337016 46.321664 \r\nL 202.946963 46.321664 \r\nL 203.55691 47.705533 \r\nL 204.166858 47.705533 \r\nL 204.776805 48.460387 \r\nL 205.386752 47.076518 \r\nL 205.996699 47.453922 \r\nL 206.606646 45.944237 \r\nL 207.216593 47.705533 \r\nL 207.826541 47.328129 \r\nL 208.436488 44.308758 \r\nL 209.046435 44.182941 \r\nL 209.656382 43.428086 \r\nL 210.266329 48.586204 \r\nL 210.876276 41.540997 \r\nL 211.486224 43.553903 \r\nL 212.096171 43.93133 \r\nL 212.706118 43.93133 \r\nL 213.316065 45.818443 \r\nL 213.926012 43.050682 \r\nL 214.535959 44.937795 \r\nL 215.145907 43.428086 \r\nL 215.755854 42.421645 \r\nL 216.365801 40.911959 \r\nL 216.975748 45.566809 \r\nL 217.585695 45.692626 \r\nL 218.195642 44.560368 \r\nL 218.80559 41.792607 \r\nL 219.415537 46.195847 \r\nL 220.025484 44.686162 \r\nL 220.635431 46.699091 \r\nL 221.245378 47.076518 \r\nL 221.855325 48.33457 \r\nL 222.465273 43.176476 \r\nL 223.07522 46.321664 \r\nL 223.685167 46.070054 \r\nL 224.295114 50.221683 \r\nL 224.905061 48.08296 \r\nL 225.515008 46.573274 \r\nL 226.124956 42.547438 \r\nL 226.734903 41.41518 \r\nL 227.34485 43.805514 \r\nL 227.954797 43.302293 \r\nL 228.564744 42.170035 \r\nL 229.174691 42.673255 \r\nL 230.394586 40.157105 \r\nL 231.004533 41.41518 \r\nL 231.61448 40.408715 \r\nL 232.224427 40.660326 \r\nL 232.834374 43.050682 \r\nL 234.054269 44.434551 \r\nL 234.664216 49.718462 \r\nL 235.274163 42.295828 \r\nL 235.88411 42.799049 \r\nL 236.494057 45.063589 \r\nL 237.104005 44.308758 \r\nL 237.713952 42.170035 \r\nL 238.323899 44.057124 \r\nL 238.933846 40.786142 \r\nL 239.543793 44.182941 \r\nL 240.15374 43.805514 \r\nL 240.763688 44.057124 \r\nL 241.373635 43.805514 \r\nL 241.983582 40.031311 \r\nL 242.593529 43.428086 \r\nL 243.203476 40.282922 \r\nL 243.813423 43.67972 \r\nL 244.423371 44.434551 \r\nL 245.033318 41.540997 \r\nL 245.643265 42.421645 \r\nL 246.253212 40.282922 \r\nL 246.863159 45.063589 \r\nL 247.473106 41.792607 \r\nL 248.083054 43.93133 \r\nL 248.693001 41.540997 \r\nL 249.302948 43.67972 \r\nL 249.912895 43.302293 \r\nL 251.132789 39.402274 \r\nL 251.742737 42.673255 \r\nL 252.352684 45.189406 \r\nL 252.962631 41.792607 \r\nL 253.572578 43.302293 \r\nL 254.182525 41.037753 \r\nL 254.792472 44.182941 \r\nL 255.40242 45.692626 \r\nL 256.012367 40.786142 \r\nL 256.622314 48.08296 \r\nL 257.232261 47.453922 \r\nL 257.842208 48.460387 \r\nL 258.452155 46.070054 \r\nL 259.062103 42.547438 \r\nL 259.67205 43.93133 \r\nL 260.281997 40.786142 \r\nL 260.891944 40.786142 \r\nL 261.501891 43.553903 \r\nL 262.111838 44.057124 \r\nL 262.721786 38.144199 \r\nL 263.331733 41.918401 \r\nL 263.94168 43.302293 \r\nL 264.551627 42.799049 \r\nL 265.161574 40.786142 \r\nL 265.771521 42.924866 \r\nL 266.381469 41.41518 \r\nL 266.991416 41.41518 \r\nL 267.601363 37.766771 \r\nL 268.21131 40.660326 \r\nL 268.821257 41.66679 \r\nL 269.431204 45.189406 \r\nL 270.041152 44.937795 \r\nL 270.651099 43.805514 \r\nL 271.261046 45.189406 \r\nL 271.870993 44.434551 \r\nL 272.48094 42.170035 \r\nL 273.090887 44.308758 \r\nL 273.700835 42.421645 \r\nL 274.310782 41.918401 \r\nL 274.920729 42.547438 \r\nL 275.530676 42.170035 \r\nL 276.140623 41.037753 \r\nL 276.75057 42.924866 \r\nL 277.360518 41.792607 \r\nL 277.970465 41.792607 \r\nL 278.580412 41.918401 \r\nL 279.190359 40.660326 \r\nL 279.800306 42.547438 \r\nL 280.410253 43.176476 \r\nL 281.020201 48.837814 \r\nL 281.630148 47.957166 \r\nL 282.240095 39.779678 \r\nL 282.850042 43.302293 \r\nL 283.459989 40.911959 \r\nL 284.069936 41.540997 \r\nL 284.679884 39.779678 \r\nL 285.289831 44.560368 \r\nL 285.899778 43.176476 \r\nL 286.509725 41.16357 \r\nL 287.729619 43.302293 \r\nL 288.339567 42.295828 \r\nL 288.949514 45.566809 \r\nL 289.559461 42.170035 \r\nL 290.169408 41.037753 \r\nL 290.779355 42.421645 \r\nL 291.389302 43.302293 \r\nL 291.99925 42.547438 \r\nL 292.609197 43.302293 \r\nL 293.219144 40.786142 \r\nL 293.829091 40.408715 \r\nL 294.439038 43.050682 \r\nL 295.048985 41.16357 \r\nL 295.658933 42.673255 \r\nL 296.26888 41.41518 \r\nL 296.878827 42.673255 \r\nL 297.488774 46.321664 \r\nL 298.098721 43.428086 \r\nL 298.708668 41.66679 \r\nL 299.318616 40.408715 \r\nL 299.928563 44.434551 \r\nL 300.53851 43.176476 \r\nL 301.148457 41.41518 \r\nL 301.758404 43.302293 \r\nL 302.368351 40.031311 \r\nL 302.978299 44.686162 \r\nL 303.588246 37.389344 \r\nL 304.198193 42.170035 \r\nL 304.80814 43.93133 \r\nL 305.418087 41.41518 \r\nL 306.028034 43.302293 \r\nL 306.637982 43.93133 \r\nL 307.247929 42.547438 \r\nL 307.857876 40.786142 \r\nL 308.467823 43.67972 \r\nL 309.07777 40.157105 \r\nL 309.687717 40.660326 \r\nL 310.297665 40.911959 \r\nL 310.907612 40.786142 \r\nL 311.517559 43.805514 \r\nL 312.127506 43.93133 \r\nL 312.737453 42.799049 \r\nL 313.3474 41.918401 \r\nL 313.957348 42.044218 \r\nL 314.567295 39.276457 \r\nL 315.177242 38.647419 \r\nL 315.787189 41.289363 \r\nL 316.397136 40.031311 \r\nL 317.007083 42.799049 \r\nL 317.617031 39.402274 \r\nL 318.226978 39.15064 \r\nL 318.836925 40.786142 \r\nL 319.446872 42.799049 \r\nL 320.056819 37.515161 \r\nL 320.666766 37.766771 \r\nL 321.276714 38.773236 \r\nL 321.886661 40.534532 \r\nL 322.496608 41.289363 \r\nL 323.106555 41.41518 \r\nL 323.716502 42.044218 \r\nL 324.326449 39.402274 \r\nL 324.936397 40.408715 \r\nL 325.546344 38.018382 \r\nL 326.156291 40.282922 \r\nL 326.766238 40.911959 \r\nL 327.376185 40.534532 \r\nL 327.986132 45.441016 \r\nL 328.59608 44.811978 \r\nL 329.206027 39.905494 \r\nL 330.425921 43.428086 \r\nL 331.035868 41.792607 \r\nL 331.645815 41.037753 \r\nL 332.255763 40.660326 \r\nL 332.86571 41.66679 \r\nL 333.475657 42.295828 \r\nL 334.085604 38.773236 \r\nL 334.695551 41.41518 \r\nL 335.305498 41.41518 \r\nL 335.915446 38.89903 \r\nL 336.525393 40.157105 \r\nL 337.13534 42.799049 \r\nL 337.745287 40.534532 \r\nL 338.355234 40.786142 \r\nL 338.965181 39.528067 \r\nL 339.575129 39.905494 \r\nL 340.185076 41.918401 \r\nL 340.795023 41.037753 \r\nL 342.014917 44.560368 \r\nL 342.624865 44.308758 \r\nL 343.234812 39.276457 \r\nL 343.844759 43.93133 \r\nL 344.454706 40.408715 \r\nL 345.064653 41.16357 \r\nL 345.6746 43.428086 \r\nL 346.284548 43.176476 \r\nL 346.894495 46.070054 \r\nL 347.504442 40.911959 \r\nL 348.114389 40.534532 \r\nL 348.724336 43.050682 \r\nL 349.944231 40.408715 \r\nL 350.554178 41.540997 \r\nL 351.164125 41.792607 \r\nL 351.774072 40.282922 \r\nL 352.384019 43.050682 \r\nL 352.993966 42.295828 \r\nL 353.603914 38.773236 \r\nL 354.213861 41.918401 \r\nL 354.823808 39.653884 \r\nL 355.433755 39.653884 \r\nL 356.043702 39.779678 \r\nL 356.653649 39.779678 \r\nL 357.263597 40.408715 \r\nL 357.873544 38.395809 \r\nL 358.483491 41.792607 \r\nL 359.093438 40.282922 \r\nL 359.703385 40.911959 \r\nL 360.313332 40.282922 \r\nL 360.92328 39.905494 \r\nL 361.533227 38.89903 \r\nL 362.143174 42.547438 \r\nL 362.753121 41.037753 \r\nL 363.363068 40.534532 \r\nL 363.363068 40.534532 \r\n\" style=\"fill:none;stroke:#0000ff;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"line2d_14\">\r\n    <path clip-path=\"url(#p3bd1476cc0)\" d=\"M 58.999432 200.621642 \r\nL 62.659115 200.621642 \r\nL 63.269062 161.910214 \r\nL 63.879009 175.484349 \r\nL 64.488956 151.855278 \r\nL 65.098903 147.833327 \r\nL 66.318798 136.270157 \r\nL 66.928745 135.264675 \r\nL 67.538692 115.154849 \r\nL 68.148639 117.668566 \r\nL 68.758586 95.547753 \r\nL 69.368534 93.536766 \r\nL 69.978481 89.012062 \r\nL 70.588428 86.498323 \r\nL 71.198375 80.465384 \r\nL 71.808322 84.990088 \r\nL 73.028217 71.415954 \r\nL 74.248111 76.44341 \r\nL 76.077952 67.896732 \r\nL 76.6879 63.372005 \r\nL 77.297847 66.388497 \r\nL 77.907794 75.940657 \r\nL 78.517741 69.907719 \r\nL 79.127688 65.885745 \r\nL 79.737635 68.399484 \r\nL 80.347583 62.869275 \r\nL 80.95753 64.37751 \r\nL 81.567477 68.399484 \r\nL 82.177424 74.935175 \r\nL 82.787371 70.913201 \r\nL 83.397319 57.841819 \r\nL 84.007266 56.836314 \r\nL 84.617213 55.328079 \r\nL 85.22716 52.814339 \r\nL 85.837107 56.836314 \r\nL 86.447054 49.79787 \r\nL 87.057002 68.902214 \r\nL 87.666949 50.803376 \r\nL 88.276896 58.344548 \r\nL 89.49679 62.366523 \r\nL 90.106737 53.317092 \r\nL 90.716685 52.31161 \r\nL 91.326632 58.344548 \r\nL 91.936579 59.350054 \r\nL 92.546526 50.300623 \r\nL 93.156473 52.814339 \r\nL 93.76642 50.803376 \r\nL 94.376368 52.814339 \r\nL 94.986315 45.273167 \r\nL 95.596262 61.361041 \r\nL 96.206209 44.267661 \r\nL 96.816156 44.267661 \r\nL 97.426103 50.300623 \r\nL 98.036051 52.31161 \r\nL 98.645998 43.764932 \r\nL 99.255945 42.256697 \r\nL 99.865892 51.808857 \r\nL 100.475839 48.792388 \r\nL 101.085786 55.328079 \r\nL 101.695734 58.344548 \r\nL 102.305681 42.256697 \r\nL 102.915628 51.306105 \r\nL 103.525575 40.24571 \r\nL 104.135522 59.350054 \r\nL 104.745469 65.382992 \r\nL 105.355417 49.79787 \r\nL 105.965364 42.256697 \r\nL 107.185258 41.251192 \r\nL 107.795205 41.753945 \r\nL 108.405152 40.748439 \r\nL 109.0151 46.781401 \r\nL 109.625047 43.262179 \r\nL 110.234994 47.786883 \r\nL 110.844941 40.24571 \r\nL 111.454888 51.306105 \r\nL 112.064835 47.284154 \r\nL 112.674783 50.803376 \r\nL 113.28473 40.24571 \r\nL 113.894677 42.759427 \r\nL 114.504624 46.781401 \r\nL 115.114571 41.753945 \r\nL 115.724518 70.410448 \r\nL 116.334466 49.79787 \r\nL 116.944413 42.256697 \r\nL 117.55436 42.759427 \r\nL 118.164307 45.273167 \r\nL 118.774254 46.278648 \r\nL 119.384201 39.742958 \r\nL 119.994149 57.339066 \r\nL 120.604096 42.256697 \r\nL 121.214043 49.79787 \r\nL 121.82399 52.31161 \r\nL 122.433937 47.284154 \r\nL 123.043884 37.73197 \r\nL 123.653832 39.240205 \r\nL 124.263779 43.262179 \r\nL 124.873726 48.792388 \r\nL 126.09362 37.73197 \r\nL 126.703567 42.759427 \r\nL 127.313515 41.251192 \r\nL 127.923462 35.218254 \r\nL 128.533409 40.24571 \r\nL 129.143356 51.808857 \r\nL 129.753303 56.333561 \r\nL 130.36325 42.256697 \r\nL 130.973198 59.852783 \r\nL 131.583145 43.262179 \r\nL 132.193092 40.24571 \r\nL 132.803039 42.759427 \r\nL 133.412986 39.240205 \r\nL 134.022933 45.273167 \r\nL 134.632881 45.775919 \r\nL 135.242828 51.306105 \r\nL 135.852775 55.328079 \r\nL 136.462722 39.240205 \r\nL 137.072669 41.251192 \r\nL 137.682616 38.234723 \r\nL 138.292564 38.737476 \r\nL 138.902511 41.753945 \r\nL 139.512458 36.726488 \r\nL 140.122405 48.289636 \r\nL 140.732352 52.814339 \r\nL 141.342299 37.229218 \r\nL 141.952247 44.267661 \r\nL 142.562194 36.726488 \r\nL 143.172141 44.267661 \r\nL 143.782088 36.223736 \r\nL 144.392035 39.742958 \r\nL 145.001982 53.317092 \r\nL 145.61193 46.278648 \r\nL 146.221877 52.814339 \r\nL 146.831824 39.240205 \r\nL 147.441771 40.748439 \r\nL 148.051718 42.759427 \r\nL 148.661665 41.251192 \r\nL 149.271613 37.229218 \r\nL 149.88156 55.830832 \r\nL 150.491507 40.748439 \r\nL 151.101454 36.726488 \r\nL 151.711401 38.234723 \r\nL 152.321348 40.748439 \r\nL 152.931296 48.792388 \r\nL 153.541243 54.825327 \r\nL 154.15119 43.262179 \r\nL 154.761137 37.229218 \r\nL 155.371084 39.742958 \r\nL 156.590979 35.720983 \r\nL 157.200926 39.742958 \r\nL 157.810873 41.753945 \r\nL 158.42082 38.234723 \r\nL 159.030767 36.726488 \r\nL 159.640714 37.229218 \r\nL 160.250662 39.240205 \r\nL 160.860609 36.726488 \r\nL 161.470556 36.726488 \r\nL 162.080503 37.229218 \r\nL 162.69045 42.759427 \r\nL 163.300397 36.726488 \r\nL 163.910345 41.753945 \r\nL 164.520292 35.218254 \r\nL 165.130239 36.726488 \r\nL 165.740186 34.715501 \r\nL 166.350133 33.709996 \r\nL 166.96008 35.218254 \r\nL 167.570028 34.212749 \r\nL 168.179975 35.720983 \r\nL 168.789922 34.715501 \r\nL 169.399869 41.251192 \r\nL 170.009816 38.234723 \r\nL 170.619763 40.24571 \r\nL 171.229711 49.295141 \r\nL 171.839658 54.322597 \r\nL 172.449605 36.223736 \r\nL 173.059552 37.73197 \r\nL 173.669499 38.234723 \r\nL 174.279446 40.748439 \r\nL 174.889394 37.229218 \r\nL 175.499341 43.262179 \r\nL 176.109288 51.808857 \r\nL 176.719235 45.775919 \r\nL 177.329182 44.770414 \r\nL 177.939129 39.240205 \r\nL 178.549077 36.223736 \r\nL 179.159024 39.742958 \r\nL 179.768971 35.720983 \r\nL 180.378918 36.726488 \r\nL 180.988865 41.753945 \r\nL 181.598812 48.792388 \r\nL 182.20876 40.748439 \r\nL 182.818707 40.748439 \r\nL 183.428654 36.223736 \r\nL 184.038601 36.223736 \r\nL 184.648548 41.753945 \r\nL 185.258495 38.234723 \r\nL 185.868443 33.709996 \r\nL 186.47839 41.251192 \r\nL 187.088337 40.24571 \r\nL 187.698284 35.720983 \r\nL 188.918178 35.720983 \r\nL 189.528126 38.234723 \r\nL 190.138073 43.764932 \r\nL 190.74802 39.240205 \r\nL 191.357967 37.73197 \r\nL 191.967914 36.726488 \r\nL 192.577861 39.240205 \r\nL 193.187809 46.781401 \r\nL 193.797756 45.273167 \r\nL 194.407703 49.79787 \r\nL 195.01765 39.742958 \r\nL 195.627597 41.251192 \r\nL 196.237544 46.278648 \r\nL 196.847492 35.218254 \r\nL 197.457439 36.726488 \r\nL 198.067386 43.764932 \r\nL 198.677333 40.24571 \r\nL 199.28728 34.715501 \r\nL 199.897227 37.229218 \r\nL 200.507175 52.814339 \r\nL 201.117122 35.720983 \r\nL 201.727069 42.256697 \r\nL 202.337016 39.240205 \r\nL 202.946963 38.234723 \r\nL 203.55691 40.748439 \r\nL 204.166858 53.819845 \r\nL 204.776805 39.742958 \r\nL 205.386752 47.786883 \r\nL 206.606646 32.201761 \r\nL 207.216593 38.737476 \r\nL 207.826541 40.24571 \r\nL 208.436488 36.726488 \r\nL 209.656382 46.781401 \r\nL 210.266329 42.759427 \r\nL 210.876276 36.223736 \r\nL 211.486224 38.737476 \r\nL 212.096171 40.24571 \r\nL 212.706118 36.726488 \r\nL 213.316065 41.251192 \r\nL 213.926012 38.234723 \r\nL 214.535959 39.742958 \r\nL 215.145907 38.737476 \r\nL 215.755854 32.704514 \r\nL 216.365801 48.289636 \r\nL 216.975748 45.273167 \r\nL 217.585695 36.223736 \r\nL 218.80559 37.229218 \r\nL 219.415537 39.742958 \r\nL 220.025484 39.742958 \r\nL 220.635431 40.24571 \r\nL 221.245378 46.781401 \r\nL 221.855325 36.726488 \r\nL 222.465273 39.742958 \r\nL 223.07522 39.240205 \r\nL 223.685167 57.339066 \r\nL 224.295114 45.775919 \r\nL 224.905061 37.229218 \r\nL 225.515008 37.73197 \r\nL 226.124956 39.240205 \r\nL 226.734903 42.256697 \r\nL 227.34485 35.218254 \r\nL 227.954797 34.212749 \r\nL 228.564744 35.218254 \r\nL 229.174691 40.748439 \r\nL 230.394586 37.73197 \r\nL 231.004533 35.218254 \r\nL 232.224427 37.229218 \r\nL 232.834374 38.737476 \r\nL 233.444322 38.737476 \r\nL 234.054269 46.781401 \r\nL 234.664216 38.737476 \r\nL 235.274163 43.262179 \r\nL 235.88411 36.726488 \r\nL 236.494057 45.273167 \r\nL 237.104005 34.715501 \r\nL 237.713952 37.229218 \r\nL 238.323899 39.240205 \r\nL 238.933846 39.240205 \r\nL 239.543793 42.256697 \r\nL 240.15374 39.240205 \r\nL 240.763688 33.709996 \r\nL 241.983582 43.764932 \r\nL 242.593529 37.73197 \r\nL 243.203476 35.218254 \r\nL 243.813423 37.229218 \r\nL 244.423371 37.73197 \r\nL 245.033318 36.726488 \r\nL 245.643265 34.212749 \r\nL 246.253212 34.212749 \r\nL 246.863159 36.223736 \r\nL 247.473106 39.240205 \r\nL 248.083054 37.229218 \r\nL 248.693001 38.737476 \r\nL 249.302948 41.251192 \r\nL 250.522842 34.715501 \r\nL 251.132789 38.234723 \r\nL 251.742737 53.819845 \r\nL 252.352684 40.748439 \r\nL 252.962631 37.229218 \r\nL 253.572578 36.726488 \r\nL 254.182525 43.262179 \r\nL 254.792472 43.262179 \r\nL 255.40242 35.720983 \r\nL 256.012367 39.240205 \r\nL 257.232261 47.786883 \r\nL 257.842208 37.73197 \r\nL 258.452155 36.726488 \r\nL 259.062103 36.726488 \r\nL 259.67205 37.73197 \r\nL 260.281997 34.212749 \r\nL 260.891944 36.223736 \r\nL 261.501891 37.229218 \r\nL 262.111838 36.223736 \r\nL 262.721786 36.726488 \r\nL 263.331733 36.726488 \r\nL 263.94168 34.212749 \r\nL 264.551627 41.753945 \r\nL 265.161574 35.218254 \r\nL 265.771521 41.753945 \r\nL 266.381469 34.212749 \r\nL 266.991416 33.709996 \r\nL 267.601363 35.218254 \r\nL 268.21131 33.207267 \r\nL 268.821257 44.770414 \r\nL 269.431204 33.709996 \r\nL 270.041152 39.240205 \r\nL 270.651099 35.720983 \r\nL 271.261046 39.240205 \r\nL 271.870993 44.267661 \r\nL 272.48094 35.218254 \r\nL 273.090887 41.753945 \r\nL 273.700835 33.207267 \r\nL 274.310782 41.251192 \r\nL 274.920729 45.273167 \r\nL 275.530676 43.262179 \r\nL 276.140623 36.726488 \r\nL 276.75057 33.207267 \r\nL 277.360518 36.223736 \r\nL 277.970465 36.223736 \r\nL 278.580412 35.218254 \r\nL 279.190359 33.709996 \r\nL 279.800306 38.234723 \r\nL 280.410253 49.295141 \r\nL 281.020201 37.229218 \r\nL 281.630148 38.737476 \r\nL 282.240095 42.256697 \r\nL 282.850042 40.24571 \r\nL 283.459989 35.720983 \r\nL 284.069936 39.240205 \r\nL 284.679884 37.229218 \r\nL 285.289831 38.737476 \r\nL 285.899778 34.715501 \r\nL 286.509725 39.240205 \r\nL 287.119672 36.223736 \r\nL 287.729619 35.720983 \r\nL 288.339567 39.742958 \r\nL 288.949514 38.737476 \r\nL 289.559461 44.267661 \r\nL 290.169408 38.234723 \r\nL 290.779355 42.759427 \r\nL 291.389302 39.240205 \r\nL 291.99925 39.742958 \r\nL 292.609197 38.234723 \r\nL 293.219144 37.229218 \r\nL 293.829091 37.229218 \r\nL 294.439038 35.720983 \r\nL 295.658933 45.775919 \r\nL 296.26888 42.759427 \r\nL 296.878827 38.737476 \r\nL 297.488774 37.229218 \r\nL 298.098721 35.218254 \r\nL 298.708668 36.223736 \r\nL 299.318616 51.808857 \r\nL 299.928563 36.726488 \r\nL 300.53851 34.212749 \r\nL 301.148457 35.218254 \r\nL 301.758404 35.720983 \r\nL 302.368351 51.808857 \r\nL 302.978299 41.753945 \r\nL 303.588246 34.715501 \r\nL 304.198193 34.212749 \r\nL 304.80814 39.240205 \r\nL 305.418087 35.218254 \r\nL 306.637982 36.223736 \r\nL 307.247929 34.715501 \r\nL 307.857876 48.792388 \r\nL 308.467823 36.223736 \r\nL 309.07777 33.207267 \r\nL 309.687717 33.207267 \r\nL 310.297665 36.223736 \r\nL 310.907612 43.262179 \r\nL 311.517559 48.792388 \r\nL 312.127506 40.748439 \r\nL 313.3474 34.715501 \r\nL 313.957348 35.218254 \r\nL 314.567295 34.212749 \r\nL 315.177242 49.79787 \r\nL 315.787189 37.73197 \r\nL 316.397136 44.770414 \r\nL 317.007083 38.737476 \r\nL 317.617031 36.223736 \r\nL 318.226978 38.234723 \r\nL 318.836925 46.278648 \r\nL 319.446872 33.709996 \r\nL 320.056819 34.212749 \r\nL 320.666766 34.212749 \r\nL 321.276714 38.234723 \r\nL 321.886661 35.218254 \r\nL 322.496608 36.223736 \r\nL 323.106555 41.753945 \r\nL 323.716502 51.306105 \r\nL 324.326449 35.218254 \r\nL 324.936397 39.240205 \r\nL 325.546344 35.720983 \r\nL 326.156291 40.24571 \r\nL 326.766238 34.715501 \r\nL 327.376185 38.234723 \r\nL 327.986132 38.234723 \r\nL 328.59608 34.715501 \r\nL 329.206027 37.229218 \r\nL 329.815974 42.759427 \r\nL 330.425921 41.753945 \r\nL 331.035868 35.720983 \r\nL 331.645815 39.742958 \r\nL 332.255763 34.212749 \r\nL 332.86571 40.748439 \r\nL 333.475657 35.218254 \r\nL 334.085604 39.742958 \r\nL 334.695551 35.720983 \r\nL 335.305498 37.229218 \r\nL 335.915446 34.715501 \r\nL 336.525393 36.726488 \r\nL 337.13534 35.720983 \r\nL 337.745287 39.240205 \r\nL 338.355234 39.240205 \r\nL 338.965181 34.212749 \r\nL 339.575129 38.234723 \r\nL 340.185076 37.73197 \r\nL 340.795023 49.295141 \r\nL 341.40497 50.803376 \r\nL 342.014917 53.317092 \r\nL 342.624865 34.715501 \r\nL 343.844759 35.720983 \r\nL 344.454706 46.278648 \r\nL 345.6746 36.726488 \r\nL 346.284548 42.759427 \r\nL 346.894495 39.240205 \r\nL 347.504442 41.753945 \r\nL 348.114389 37.73197 \r\nL 348.724336 42.256697 \r\nL 349.334283 38.737476 \r\nL 349.944231 37.73197 \r\nL 350.554178 40.748439 \r\nL 351.164125 40.748439 \r\nL 351.774072 42.256697 \r\nL 352.384019 39.240205 \r\nL 352.993966 34.715501 \r\nL 354.823808 39.240205 \r\nL 355.433755 43.262179 \r\nL 356.043702 41.753945 \r\nL 356.653649 34.212749 \r\nL 357.263597 38.737476 \r\nL 357.873544 38.234723 \r\nL 358.483491 35.720983 \r\nL 359.093438 34.715501 \r\nL 359.703385 36.726488 \r\nL 360.313332 33.709996 \r\nL 360.92328 44.770414 \r\nL 361.533227 38.234723 \r\nL 362.143174 38.737476 \r\nL 362.753121 36.726488 \r\nL 363.363068 36.223736 \r\nL 363.363068 36.223736 \r\n\" style=\"fill:none;stroke:#ff0000;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 43.78125 239.758125 \r\nL 43.78125 22.318125 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 378.58125 239.758125 \r\nL 378.58125 22.318125 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 43.78125 239.758125 \r\nL 378.58125 239.758125 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 43.78125 22.318125 \r\nL 378.58125 22.318125 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"text_15\">\r\n    <!-- Training and validation Acc -->\r\n    <defs>\r\n     <path d=\"M -0.296875 72.90625 \r\nL 61.375 72.90625 \r\nL 61.375 64.59375 \r\nL 35.5 64.59375 \r\nL 35.5 0 \r\nL 25.59375 0 \r\nL 25.59375 64.59375 \r\nL -0.296875 64.59375 \r\nz\r\n\" id=\"DejaVuSans-84\"/>\r\n     <path d=\"M 41.109375 46.296875 \r\nQ 39.59375 47.171875 37.8125 47.578125 \r\nQ 36.03125 48 33.890625 48 \r\nQ 26.265625 48 22.1875 43.046875 \r\nQ 18.109375 38.09375 18.109375 28.8125 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 20.953125 51.171875 25.484375 53.578125 \r\nQ 30.03125 56 36.53125 56 \r\nQ 37.453125 56 38.578125 55.875 \r\nQ 39.703125 55.765625 41.0625 55.515625 \r\nz\r\n\" id=\"DejaVuSans-114\"/>\r\n     <path d=\"M 34.28125 27.484375 \r\nQ 23.390625 27.484375 19.1875 25 \r\nQ 14.984375 22.515625 14.984375 16.5 \r\nQ 14.984375 11.71875 18.140625 8.90625 \r\nQ 21.296875 6.109375 26.703125 6.109375 \r\nQ 34.1875 6.109375 38.703125 11.40625 \r\nQ 43.21875 16.703125 43.21875 25.484375 \r\nL 43.21875 27.484375 \r\nz\r\nM 52.203125 31.203125 \r\nL 52.203125 0 \r\nL 43.21875 0 \r\nL 43.21875 8.296875 \r\nQ 40.140625 3.328125 35.546875 0.953125 \r\nQ 30.953125 -1.421875 24.3125 -1.421875 \r\nQ 15.921875 -1.421875 10.953125 3.296875 \r\nQ 6 8.015625 6 15.921875 \r\nQ 6 25.140625 12.171875 29.828125 \r\nQ 18.359375 34.515625 30.609375 34.515625 \r\nL 43.21875 34.515625 \r\nL 43.21875 35.40625 \r\nQ 43.21875 41.609375 39.140625 45 \r\nQ 35.0625 48.390625 27.6875 48.390625 \r\nQ 23 48.390625 18.546875 47.265625 \r\nQ 14.109375 46.140625 10.015625 43.890625 \r\nL 10.015625 52.203125 \r\nQ 14.9375 54.109375 19.578125 55.046875 \r\nQ 24.21875 56 28.609375 56 \r\nQ 40.484375 56 46.34375 49.84375 \r\nQ 52.203125 43.703125 52.203125 31.203125 \r\nz\r\n\" id=\"DejaVuSans-97\"/>\r\n     <path d=\"M 9.421875 54.6875 \r\nL 18.40625 54.6875 \r\nL 18.40625 0 \r\nL 9.421875 0 \r\nz\r\nM 9.421875 75.984375 \r\nL 18.40625 75.984375 \r\nL 18.40625 64.59375 \r\nL 9.421875 64.59375 \r\nz\r\n\" id=\"DejaVuSans-105\"/>\r\n     <path d=\"M 54.890625 33.015625 \r\nL 54.890625 0 \r\nL 45.90625 0 \r\nL 45.90625 32.71875 \r\nQ 45.90625 40.484375 42.875 44.328125 \r\nQ 39.84375 48.1875 33.796875 48.1875 \r\nQ 26.515625 48.1875 22.3125 43.546875 \r\nQ 18.109375 38.921875 18.109375 30.90625 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 21.34375 51.125 25.703125 53.5625 \r\nQ 30.078125 56 35.796875 56 \r\nQ 45.21875 56 50.046875 50.171875 \r\nQ 54.890625 44.34375 54.890625 33.015625 \r\nz\r\n\" id=\"DejaVuSans-110\"/>\r\n     <path d=\"M 45.40625 27.984375 \r\nQ 45.40625 37.75 41.375 43.109375 \r\nQ 37.359375 48.484375 30.078125 48.484375 \r\nQ 22.859375 48.484375 18.828125 43.109375 \r\nQ 14.796875 37.75 14.796875 27.984375 \r\nQ 14.796875 18.265625 18.828125 12.890625 \r\nQ 22.859375 7.515625 30.078125 7.515625 \r\nQ 37.359375 7.515625 41.375 12.890625 \r\nQ 45.40625 18.265625 45.40625 27.984375 \r\nz\r\nM 54.390625 6.78125 \r\nQ 54.390625 -7.171875 48.1875 -13.984375 \r\nQ 42 -20.796875 29.203125 -20.796875 \r\nQ 24.46875 -20.796875 20.265625 -20.09375 \r\nQ 16.0625 -19.390625 12.109375 -17.921875 \r\nL 12.109375 -9.1875 \r\nQ 16.0625 -11.328125 19.921875 -12.34375 \r\nQ 23.78125 -13.375 27.78125 -13.375 \r\nQ 36.625 -13.375 41.015625 -8.765625 \r\nQ 45.40625 -4.15625 45.40625 5.171875 \r\nL 45.40625 9.625 \r\nQ 42.625 4.78125 38.28125 2.390625 \r\nQ 33.9375 0 27.875 0 \r\nQ 17.828125 0 11.671875 7.65625 \r\nQ 5.515625 15.328125 5.515625 27.984375 \r\nQ 5.515625 40.671875 11.671875 48.328125 \r\nQ 17.828125 56 27.875 56 \r\nQ 33.9375 56 38.28125 53.609375 \r\nQ 42.625 51.21875 45.40625 46.390625 \r\nL 45.40625 54.6875 \r\nL 54.390625 54.6875 \r\nz\r\n\" id=\"DejaVuSans-103\"/>\r\n     <path id=\"DejaVuSans-32\"/>\r\n     <path d=\"M 45.40625 46.390625 \r\nL 45.40625 75.984375 \r\nL 54.390625 75.984375 \r\nL 54.390625 0 \r\nL 45.40625 0 \r\nL 45.40625 8.203125 \r\nQ 42.578125 3.328125 38.25 0.953125 \r\nQ 33.9375 -1.421875 27.875 -1.421875 \r\nQ 17.96875 -1.421875 11.734375 6.484375 \r\nQ 5.515625 14.40625 5.515625 27.296875 \r\nQ 5.515625 40.1875 11.734375 48.09375 \r\nQ 17.96875 56 27.875 56 \r\nQ 33.9375 56 38.25 53.625 \r\nQ 42.578125 51.265625 45.40625 46.390625 \r\nz\r\nM 14.796875 27.296875 \r\nQ 14.796875 17.390625 18.875 11.75 \r\nQ 22.953125 6.109375 30.078125 6.109375 \r\nQ 37.203125 6.109375 41.296875 11.75 \r\nQ 45.40625 17.390625 45.40625 27.296875 \r\nQ 45.40625 37.203125 41.296875 42.84375 \r\nQ 37.203125 48.484375 30.078125 48.484375 \r\nQ 22.953125 48.484375 18.875 42.84375 \r\nQ 14.796875 37.203125 14.796875 27.296875 \r\nz\r\n\" id=\"DejaVuSans-100\"/>\r\n     <path d=\"M 2.984375 54.6875 \r\nL 12.5 54.6875 \r\nL 29.59375 8.796875 \r\nL 46.6875 54.6875 \r\nL 56.203125 54.6875 \r\nL 35.6875 0 \r\nL 23.484375 0 \r\nz\r\n\" id=\"DejaVuSans-118\"/>\r\n     <path d=\"M 9.421875 75.984375 \r\nL 18.40625 75.984375 \r\nL 18.40625 0 \r\nL 9.421875 0 \r\nz\r\n\" id=\"DejaVuSans-108\"/>\r\n     <path d=\"M 18.3125 70.21875 \r\nL 18.3125 54.6875 \r\nL 36.8125 54.6875 \r\nL 36.8125 47.703125 \r\nL 18.3125 47.703125 \r\nL 18.3125 18.015625 \r\nQ 18.3125 11.328125 20.140625 9.421875 \r\nQ 21.96875 7.515625 27.59375 7.515625 \r\nL 36.8125 7.515625 \r\nL 36.8125 0 \r\nL 27.59375 0 \r\nQ 17.1875 0 13.234375 3.875 \r\nQ 9.28125 7.765625 9.28125 18.015625 \r\nL 9.28125 47.703125 \r\nL 2.6875 47.703125 \r\nL 2.6875 54.6875 \r\nL 9.28125 54.6875 \r\nL 9.28125 70.21875 \r\nz\r\n\" id=\"DejaVuSans-116\"/>\r\n    </defs>\r\n    <g transform=\"translate(129.387188 16.318125)scale(0.12 -0.12)\">\r\n     <use xlink:href=\"#DejaVuSans-84\"/>\r\n     <use x=\"60.865234\" xlink:href=\"#DejaVuSans-114\"/>\r\n     <use x=\"101.978516\" xlink:href=\"#DejaVuSans-97\"/>\r\n     <use x=\"163.257812\" xlink:href=\"#DejaVuSans-105\"/>\r\n     <use x=\"191.041016\" xlink:href=\"#DejaVuSans-110\"/>\r\n     <use x=\"254.419922\" xlink:href=\"#DejaVuSans-105\"/>\r\n     <use x=\"282.203125\" xlink:href=\"#DejaVuSans-110\"/>\r\n     <use x=\"345.582031\" xlink:href=\"#DejaVuSans-103\"/>\r\n     <use x=\"409.058594\" xlink:href=\"#DejaVuSans-32\"/>\r\n     <use x=\"440.845703\" xlink:href=\"#DejaVuSans-97\"/>\r\n     <use x=\"502.125\" xlink:href=\"#DejaVuSans-110\"/>\r\n     <use x=\"565.503906\" xlink:href=\"#DejaVuSans-100\"/>\r\n     <use x=\"628.980469\" xlink:href=\"#DejaVuSans-32\"/>\r\n     <use x=\"660.767578\" xlink:href=\"#DejaVuSans-118\"/>\r\n     <use x=\"719.947266\" xlink:href=\"#DejaVuSans-97\"/>\r\n     <use x=\"781.226562\" xlink:href=\"#DejaVuSans-108\"/>\r\n     <use x=\"809.009766\" xlink:href=\"#DejaVuSans-105\"/>\r\n     <use x=\"836.792969\" xlink:href=\"#DejaVuSans-100\"/>\r\n     <use x=\"900.269531\" xlink:href=\"#DejaVuSans-97\"/>\r\n     <use x=\"961.548828\" xlink:href=\"#DejaVuSans-116\"/>\r\n     <use x=\"1000.757812\" xlink:href=\"#DejaVuSans-105\"/>\r\n     <use x=\"1028.541016\" xlink:href=\"#DejaVuSans-111\"/>\r\n     <use x=\"1089.722656\" xlink:href=\"#DejaVuSans-110\"/>\r\n     <use x=\"1153.101562\" xlink:href=\"#DejaVuSans-32\"/>\r\n     <use x=\"1184.888672\" xlink:href=\"#DejaVuSans-65\"/>\r\n     <use x=\"1253.28125\" xlink:href=\"#DejaVuSans-99\"/>\r\n     <use x=\"1308.261719\" xlink:href=\"#DejaVuSans-99\"/>\r\n    </g>\r\n   </g>\r\n   <g id=\"legend_1\">\r\n    <g id=\"patch_7\">\r\n     <path d=\"M 268.421875 234.758125 \r\nL 371.58125 234.758125 \r\nQ 373.58125 234.758125 373.58125 232.758125 \r\nL 373.58125 204.401875 \r\nQ 373.58125 202.401875 371.58125 202.401875 \r\nL 268.421875 202.401875 \r\nQ 266.421875 202.401875 266.421875 204.401875 \r\nL 266.421875 232.758125 \r\nQ 266.421875 234.758125 268.421875 234.758125 \r\nz\r\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\r\n    </g>\r\n    <g id=\"line2d_15\">\r\n     <path d=\"M 270.421875 210.500312 \r\nL 290.421875 210.500312 \r\n\" style=\"fill:none;stroke:#0000ff;stroke-linecap:square;stroke-width:1.5;\"/>\r\n    </g>\r\n    <g id=\"line2d_16\"/>\r\n    <g id=\"text_16\">\r\n     <!-- Training Acc -->\r\n     <g transform=\"translate(298.421875 214.000312)scale(0.1 -0.1)\">\r\n      <use xlink:href=\"#DejaVuSans-84\"/>\r\n      <use x=\"60.865234\" xlink:href=\"#DejaVuSans-114\"/>\r\n      <use x=\"101.978516\" xlink:href=\"#DejaVuSans-97\"/>\r\n      <use x=\"163.257812\" xlink:href=\"#DejaVuSans-105\"/>\r\n      <use x=\"191.041016\" xlink:href=\"#DejaVuSans-110\"/>\r\n      <use x=\"254.419922\" xlink:href=\"#DejaVuSans-105\"/>\r\n      <use x=\"282.203125\" xlink:href=\"#DejaVuSans-110\"/>\r\n      <use x=\"345.582031\" xlink:href=\"#DejaVuSans-103\"/>\r\n      <use x=\"409.058594\" xlink:href=\"#DejaVuSans-32\"/>\r\n      <use x=\"440.845703\" xlink:href=\"#DejaVuSans-65\"/>\r\n      <use x=\"509.238281\" xlink:href=\"#DejaVuSans-99\"/>\r\n      <use x=\"564.21875\" xlink:href=\"#DejaVuSans-99\"/>\r\n     </g>\r\n    </g>\r\n    <g id=\"line2d_17\">\r\n     <path d=\"M 270.421875 225.178437 \r\nL 290.421875 225.178437 \r\n\" style=\"fill:none;stroke:#ff0000;stroke-linecap:square;stroke-width:1.5;\"/>\r\n    </g>\r\n    <g id=\"line2d_18\"/>\r\n    <g id=\"text_17\">\r\n     <!-- Validation Acc -->\r\n     <defs>\r\n      <path d=\"M 28.609375 0 \r\nL 0.78125 72.90625 \r\nL 11.078125 72.90625 \r\nL 34.1875 11.53125 \r\nL 57.328125 72.90625 \r\nL 67.578125 72.90625 \r\nL 39.796875 0 \r\nz\r\n\" id=\"DejaVuSans-86\"/>\r\n     </defs>\r\n     <g transform=\"translate(298.421875 228.678437)scale(0.1 -0.1)\">\r\n      <use xlink:href=\"#DejaVuSans-86\"/>\r\n      <use x=\"68.298828\" xlink:href=\"#DejaVuSans-97\"/>\r\n      <use x=\"129.578125\" xlink:href=\"#DejaVuSans-108\"/>\r\n      <use x=\"157.361328\" xlink:href=\"#DejaVuSans-105\"/>\r\n      <use x=\"185.144531\" xlink:href=\"#DejaVuSans-100\"/>\r\n      <use x=\"248.621094\" xlink:href=\"#DejaVuSans-97\"/>\r\n      <use x=\"309.900391\" xlink:href=\"#DejaVuSans-116\"/>\r\n      <use x=\"349.109375\" xlink:href=\"#DejaVuSans-105\"/>\r\n      <use x=\"376.892578\" xlink:href=\"#DejaVuSans-111\"/>\r\n      <use x=\"438.074219\" xlink:href=\"#DejaVuSans-110\"/>\r\n      <use x=\"501.453125\" xlink:href=\"#DejaVuSans-32\"/>\r\n      <use x=\"533.240234\" xlink:href=\"#DejaVuSans-65\"/>\r\n      <use x=\"601.632812\" xlink:href=\"#DejaVuSans-99\"/>\r\n      <use x=\"656.613281\" xlink:href=\"#DejaVuSans-99\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"p3bd1476cc0\">\r\n   <rect height=\"217.44\" width=\"334.8\" x=\"43.78125\" y=\"22.318125\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "plt.plot(epochs, acc, 'b', label='Training Acc')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation Acc')\n",
    "plt.title('Training and validation Acc')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Acc')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 1, 6) for input Tensor(\"flatten_14_input:0\", shape=(None, 1, 6), dtype=float32), but it was called on an input with incompatible shape (None, 6).\n"
     ]
    }
   ],
   "source": [
    "test_pred = model.predict_classes(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              precision    recall  f1-score   support\n\n           0       0.95      0.96      0.95       859\n           1       0.86      0.88      0.87       529\n           2       0.91      0.85      0.88       285\n\n    accuracy                           0.91      1673\n   macro avg       0.91      0.89      0.90      1673\nweighted avg       0.91      0.91      0.91      1673\n\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(encoder.inverse_transform(encoded_Y),test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}