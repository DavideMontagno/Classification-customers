{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37564bit61209d9bcfea452998e2771b9ee1a7fb",
   "display_name": "Python 3.7.5 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Random Forest"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "df = pd.read_csv(\"dataset/CL-dataset.csv\",sep='\\t',decimal=\",\",index_col=0)\n",
    "df = df.drop(columns=[\"CustomerID\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   Imax         E    PIL          Mb      MeanD     EDate  class\n",
       "0    12  4.335643  2.567  156.860294   0.000000  0.000000      2\n",
       "1    32  6.503112  2.567  356.232222  35.000000  2.226424      2\n",
       "2   360  6.504979  2.752  442.969333  24.266667  2.235084      2\n",
       "3    80  4.494680  2.567  189.650000  55.400000  1.177965      1\n",
       "4    32  0.000000  2.567  292.000000  13.000000  0.918296      1"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Imax</th>\n      <th>E</th>\n      <th>PIL</th>\n      <th>Mb</th>\n      <th>MeanD</th>\n      <th>EDate</th>\n      <th>class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>12</td>\n      <td>4.335643</td>\n      <td>2.567</td>\n      <td>156.860294</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>32</td>\n      <td>6.503112</td>\n      <td>2.567</td>\n      <td>356.232222</td>\n      <td>35.000000</td>\n      <td>2.226424</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>360</td>\n      <td>6.504979</td>\n      <td>2.752</td>\n      <td>442.969333</td>\n      <td>24.266667</td>\n      <td>2.235084</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>80</td>\n      <td>4.494680</td>\n      <td>2.567</td>\n      <td>189.650000</td>\n      <td>55.400000</td>\n      <td>1.177965</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>32</td>\n      <td>0.000000</td>\n      <td>2.567</td>\n      <td>292.000000</td>\n      <td>13.000000</td>\n      <td>0.918296</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "label = df.pop('class')\n",
    "train_set, test_set, train_label, test_label = train_test_split(df, label, stratify =label, test_size=0.30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from scipy.stats import randint as sp_randint\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "source": [
    "## Grid search"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "param_dist = [{\"max_depth\": [2,3,12,None],\n",
    "              \"max_features\": [6],# Al massimo 6 non di pi√π\n",
    "              \"min_samples_split\": [10,50],\n",
    "              \"min_samples_leaf\": [10,50],\n",
    "              \"bootstrap\": [True, False],\n",
    "              \"criterion\": [\"entropy\", \"gini\"],\n",
    "              \"class_weight\":['balanced', None, {0: 0.3, 1: 0.7}]}]\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=30)\n",
    "grid_search = GridSearchCV(clf, param_dist,  \n",
    "                            n_jobs=3, \n",
    "                            scoring='accuracy')\n",
    "\n",
    "grid_search.fit(train_set, train_label)\n",
    "print(\"Best parameters:\",grid_search.best_params_)\n",
    "print(\"Best Score: \",grid_search.best_score_)\n",
    "\n",
    "print(\"Complete Grid search:\")\n",
    "means = grid_search.cv_results_['mean_test_score']\n",
    "stds = grid_search.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, grid_search.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\"% (mean, std * 2, params))"
   ]
  },
  {
   "source": [
    "## Modello Finale"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=30, \n",
    "                             criterion='gini',\n",
    "                             max_features=6,\n",
    "                             max_depth=6, \n",
    "                             min_samples_split=32,\n",
    "                             min_samples_leaf=10,\n",
    "                             bootstrap=False) \n",
    "rf = rf.fit(train_set, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction on the test test\n",
    "test_pred_rf = rf.predict(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              precision    recall  f1-score   support\n\n           0       0.92      0.91      0.91       859\n           1       0.76      0.77      0.76       529\n           2       0.80      0.81      0.81       285\n\n    accuracy                           0.85      1673\n   macro avg       0.83      0.83      0.83      1673\nweighted avg       0.85      0.85      0.85      1673\n\n"
     ]
    }
   ],
   "source": [
    "#compute the performance of the model\n",
    "print(classification_report(test_label, \n",
    "                            test_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}